{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Optional\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import error, spaces\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from gymnasium.utils import EzPickle, colorize\n",
    "from gymnasium.utils.step_api_compatibility import step_api_compatibility\n",
    "\n",
    "import pygame\n",
    "\n",
    "import Box2D\n",
    "from Box2D.b2 import (\n",
    "    circleShape,\n",
    "    contactListener,\n",
    "    edgeShape,\n",
    "    fixtureDef,\n",
    "    polygonShape,\n",
    "    revoluteJointDef,\n",
    "    staticBody, \n",
    "    dynamicBody\n",
    ")\n",
    "\n",
    "import pygame\n",
    "\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from collections import deque\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **state Space**\n",
    "$(x, y, p_x, p_y, angle, L, mass, intertia)$ : most simplified\n",
    "\n",
    "### **observation**\n",
    "(FOV array)\n",
    "\n",
    "### **Action space**\n",
    "(Propulsion (acc in facing angle), angular velocity)\n",
    "```python\n",
    "thrust, angular_momentum = action\n",
    "thrust = np.clip(thrust, 0., 1.)\n",
    "angular_momentum = np.clip(angular_momentum, -1., 1.) * UAV_ANG_POW\n",
    "thrust_force = thrust_direction * thrust * UAV_THRUST_POW\n",
    "```\n",
    "Note the input of thrust and angular momentum needs to be interpretated as 0-100%, -100%-100%\n",
    "\n",
    "### **Rewards**\n",
    "*   increase/decrease the closer/durther the agent is to the goal.\n",
    "*   increase/decrease the larger/smaller the magnitute of velocity $||v||$.\n",
    "*   increase/decrease the larger/smaller the $\\%$ of obstacle in FOV.\n",
    "\n",
    "The episode receive an additional reward of -100 for crashing.\n",
    "\n",
    "An episode is considered a solution if it reaches score.\n",
    "\n",
    "### **Starting State**\n",
    "The agent starts at the left ceter of the viewpoirt, with a initial $v_x$\n",
    "\n",
    "### **Episode Termination**\n",
    "If:\n",
    "* The agent crashes\n",
    "* The agent gets outside of the viewport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Constants\n",
    "FPS = 50\n",
    "SCALE = 10.0  # Scaling for visualization\n",
    "OBSTACLE_SCALE = 5.0  # Scaling for obstacles\n",
    "UAV_RADIUS = 10.0 / SCALE  # Radius of the UAV\n",
    "GOAL_RADIUS = UAV_RADIUS  # Radius of the Goal\n",
    "VIEWPORT_W = 600\n",
    "VIEWPORT_H = 400\n",
    "WALL_THICKNESS = 1.0  # Thickness of the walls\n",
    "NUM_OBSTACLES = 10  # Number of obstacles\n",
    "OBS_MAX_RAD = min(VIEWPORT_W, VIEWPORT_H) / SCALE / OBSTACLE_SCALE # Maximum radius of obstacles\n",
    "OBS_MIN_RAD = OBS_MAX_RAD / 3  # Minimum radius of obstacles\n",
    "MIN_CLEARANCE = UAV_RADIUS * 3  # Minimum clearance between obstacles\n",
    "\n",
    "#Note: For the angle in box2d, 0 rad is at 3 o'clock, and positive angle is in the clockwise direction\n",
    "\n",
    "# UAV specs\n",
    "UAV_INI_ANGLE = np.deg2rad(0)\n",
    "UAV_DENSITY = 1.0\n",
    "UAV_FRICTION = 0.3\n",
    "UAV_FOV = np.deg2rad(90)  # Field of View in degrees\n",
    "UAV_NUM_RAYS = 10  # Number of rays in the FOV array\n",
    "UAV_FOV_DISTANCE = 100 / SCALE  # Maximum sensing distance\n",
    "UAV_ANG_POW = 10  # Maximum angular velocity (deg/s?)\n",
    "UAV_THRUST_POW = 10  # Maximum thrust (Unit/s?)\n",
    "\n",
    "# Penalty/Reward coeff\n",
    "PEN_THRUST = -0.1\n",
    "PEN_ANG = -0.01\n",
    "PEN_OBSTACLE = -1\n",
    "PEN_COLLISION = -100\n",
    "REW_VEL = 1\n",
    "REW_ANGLE = 0.3\n",
    "REW_GOAL = 100\n",
    "REW_DIST2GOAL = 10\n",
    "\n",
    "class ContactDetector(Box2D.b2ContactListener):\n",
    "    def __init__(self, env):\n",
    "        super(ContactDetector, self).__init__()\n",
    "        self.env = env\n",
    "\n",
    "    def BeginContact(self, contact):\n",
    "        # Check if one of the bodies is the UAV and the other is an obstacle or wall\n",
    "        if contact.fixtureA.body == self.env.uav or contact.fixtureB.body == self.env.uav:\n",
    "            self.env.game_over = True\n",
    "\n",
    "class SimpleUAVEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': FPS}\n",
    "\n",
    "    def __init__(self):\n",
    "        # Define the Box2D world\n",
    "        self.world = Box2D.b2World(gravity=(0, 0))\n",
    "        self.uav = None  # UAV object\n",
    "        self.goal = None  # Goal position\n",
    "        self.dist2goal = None  # Distance to goal\n",
    "        self.walls = []  # Walls\n",
    "        self.obstacles = []  # Store properties of obstacles\n",
    "        self.obstacles_properties = []\n",
    "\n",
    "\n",
    "        # For rendering\n",
    "        self.screen = None\n",
    "        self.isopen = True\n",
    "\n",
    "    def _create_walls(self):\n",
    "        # Create walls around the viewport\n",
    "        wall_shapes = [\n",
    "            edgeShape(vertices=[(0, 0), (VIEWPORT_W/SCALE, 0)]),  # Bottom\n",
    "            edgeShape(vertices=[(0, 0), (0, VIEWPORT_H/SCALE)]),  # Left\n",
    "            edgeShape(vertices=[(0, VIEWPORT_H/SCALE), (VIEWPORT_W/SCALE, VIEWPORT_H/SCALE)]),  # Top\n",
    "            edgeShape(vertices=[(VIEWPORT_W/SCALE, 0), (VIEWPORT_W/SCALE, VIEWPORT_H/SCALE)])  # Right\n",
    "        ]\n",
    "        for shape in wall_shapes:\n",
    "            wall_body = self.world.CreateStaticBody(\n",
    "                position=(0, 0),\n",
    "                shapes=shape\n",
    "            )\n",
    "            self.walls.append(wall_body)\n",
    "\n",
    "    def _is_position_valid(self, new_properties):\n",
    "        # Check against existing obstacles\n",
    "        for prop in self.obstacles_properties:\n",
    "            distance = math.sqrt((new_properties['centroid_x'] - prop['centroid_x'])**2 + \n",
    "                                (new_properties['centroid_y'] - prop['centroid_y'])**2)\n",
    "            if distance < (new_properties['max_span'] + prop['max_span'] + MIN_CLEARANCE):\n",
    "                return False\n",
    "\n",
    "        # Check distance to the goal\n",
    "        goal_distance = math.sqrt((new_properties['centroid_x'] - self.goal[0])**2 +\n",
    "                                (new_properties['centroid_y'] - self.goal[1])**2)\n",
    "        if goal_distance < (new_properties['max_span'] + 1.5*UAV_RADIUS):\n",
    "            return False\n",
    "\n",
    "        # Check distance to the uav starting position\n",
    "        uav_distance = math.sqrt((new_properties['centroid_x'] - self.uav_start_pos[0])**2 +\n",
    "                                (new_properties['centroid_y'] - self.uav_start_pos[1])**2)\n",
    "        if uav_distance < (new_properties['max_span'] + 1.5*UAV_RADIUS):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def _generate_triangle_properties(self):\n",
    "        # Randomly generate centroid within bounds\n",
    "        centroid_x = random.uniform(UAV_RADIUS, VIEWPORT_W / SCALE - UAV_RADIUS)\n",
    "        centroid_y = random.uniform(UAV_RADIUS, VIEWPORT_H / SCALE - UAV_RADIUS)\n",
    "        \n",
    "        # Random length from centroid to vertices\n",
    "        length = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "\n",
    "        # Angle offsets for equilateral triangle vertices\n",
    "        angle_offset = math.pi * 2 / 3\n",
    "\n",
    "        # Calculate vertices based on centroid and length\n",
    "        vertices = []\n",
    "        for i in range(3):\n",
    "            angle = angle_offset * i\n",
    "            vertex_x = centroid_x + length * math.cos(angle)\n",
    "            vertex_y = centroid_y + length * math.sin(angle)\n",
    "            vertices.append((vertex_x, vertex_y))\n",
    "\n",
    "        # Random rotation angle\n",
    "        rotation_angle = random.uniform(0, math.pi)\n",
    "\n",
    "        return {'type': 'triangle', 'vertices': vertices, 'centroid_x': centroid_x, 'centroid_y': centroid_y, 'angle': rotation_angle, 'max_span': length}\n",
    "\n",
    "    def _generate_rectangle_properties(self):\n",
    "        width = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "        height = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "        angle = random.uniform(0, math.pi)  # Random angle in radians\n",
    "\n",
    "        centroid_x = random.uniform(width / 2, VIEWPORT_W / SCALE - width / 2)\n",
    "        centroid_y = random.uniform(height / 2, VIEWPORT_H / SCALE - height / 2)\n",
    "\n",
    "        return {'type': 'rectangle', 'centroid_x': centroid_x, 'centroid_y': centroid_y, 'width': width, 'height': height, 'angle': angle, 'max_span': max(width, height)}\n",
    "\n",
    "    def _generate_circle_properties(self):\n",
    "        radius = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "        centroid_x = random.uniform(radius, VIEWPORT_W / SCALE - radius)\n",
    "        centroid_y = random.uniform(radius, VIEWPORT_H / SCALE - radius)\n",
    "\n",
    "        return {'type': 'circle', 'centroid_x': centroid_x, 'centroid_y': centroid_y, 'max_span': radius}\n",
    "\n",
    "    def _create_obstacle_from_properties(self, properties):\n",
    "        if properties['type'] == 'circle':\n",
    "            body = self.world.CreateStaticBody(position=(properties['centroid_x'], properties['centroid_y']))\n",
    "            circle = body.CreateCircleFixture(radius=properties['max_span'], density=1, friction=0.3)\n",
    "            self.obstacles.append(circle)\n",
    "\n",
    "        elif properties['type'] == 'rectangle':\n",
    "            body = self.world.CreateStaticBody(position=(properties['centroid_x'], properties['centroid_y']))\n",
    "            rectangle = body.CreatePolygonFixture(box=(properties['width'] / 2, properties['height'] / 2), density=1, friction=0.3)\n",
    "            body.angle = properties['angle']\n",
    "            self.obstacles.append(rectangle)\n",
    "\n",
    "        elif properties['type'] == 'triangle':\n",
    "            vertices = [(v[0] - properties['centroid_x'], v[1] - properties['centroid_y']) for v in properties['vertices']]\n",
    "            body = self.world.CreateStaticBody(position=(properties['centroid_x'], properties['centroid_y']))\n",
    "            triangle = body.CreatePolygonFixture(vertices=vertices, density=1, friction=0.3)\n",
    "            body.angle = properties['angle']\n",
    "            self.obstacles.append(triangle)\n",
    "\n",
    "    def _create_obstacles(self, num_obstacles=5):\n",
    "        num_obstacles = num_obstacles\n",
    "        obstacle_types = ['triangle', 'rectangle', 'circle']\n",
    "        max_iter = 1000\n",
    "        for _ in range(num_obstacles):\n",
    "            obstacle_type = random.choice(obstacle_types)\n",
    "            for iter in range(max_iter):\n",
    "                if obstacle_type == 'triangle':\n",
    "                    properties = self._generate_triangle_properties()\n",
    "                elif obstacle_type == 'rectangle':\n",
    "                    properties = self._generate_rectangle_properties()\n",
    "                elif obstacle_type == 'circle':\n",
    "                    properties = self._generate_circle_properties()\n",
    "\n",
    "                # Check if the new obstacle overlaps with existing ones\n",
    "                if self._is_position_valid(properties):\n",
    "                    self.obstacles_properties.append(properties)\n",
    "                    # Create the actual obstacle based on properties\n",
    "                    #print(properties['type']+': pass checking')\n",
    "                    self._create_obstacle_from_properties(properties)\n",
    "                    break\n",
    "                #if iter == max_iter - 1:\n",
    "                #    print('Failed to create: ', obstacle_type)\n",
    "\n",
    "    def _create_uav(self):\n",
    "        # Create the UAV at a position away from the left wall\n",
    "        uav_start_pos = (UAV_RADIUS + 2 * WALL_THICKNESS, VIEWPORT_H / SCALE / 2)\n",
    "        self.uav_start_pos = uav_start_pos\n",
    "        self.uav = self.world.CreateDynamicBody(position=uav_start_pos, angle=UAV_INI_ANGLE, linearVelocity=(0,0), angularVelocity=0.0)\n",
    "        self.uav.CreateCircleFixture(radius=UAV_RADIUS, density=UAV_DENSITY, friction=UAV_FRICTION)\n",
    "\n",
    "    def _create_goal(self):\n",
    "        # Create a random goal position away from the right wall\n",
    "        goal_pos_x = VIEWPORT_W / SCALE - GOAL_RADIUS - 2 * WALL_THICKNESS\n",
    "        goal_pos_y = random.uniform(WALL_THICKNESS+UAV_RADIUS, VIEWPORT_H / SCALE - UAV_RADIUS)\n",
    "        self.goal = (goal_pos_x, goal_pos_y)\n",
    "        self.ini_to_goal_dist = math.sqrt((self.uav_start_pos[0] - self.goal[0])**2 + (self.uav_start_pos[1] - self.goal[1])**2)\n",
    "\n",
    "    def _calculate_reward(self, obs, action):\n",
    "        # Distance to goal\n",
    "        dist2goal = math.sqrt((self.uav.position.x - self.goal[0])**2 + (self.uav.position.y - self.goal[1])**2)\n",
    "        self.dist2goal = dist2goal\n",
    "        distance_reward = (1 - dist2goal / self.ini_to_goal_dist) * REW_DIST2GOAL  # Normalize \n",
    "\n",
    "        # Velocity reward\n",
    "        velocity_reward = self.uav.linearVelocity.length * REW_VEL\n",
    "\n",
    "        # Angle reward\n",
    "        self.ang2goal = (np.arctan2(self.goal[1] - self.uav.position.y, self.goal[0] - self.uav.position.x) - self.uav.angle + np.pi) % (2 * np.pi) - np.pi\n",
    "        angle_reward = (1 - abs(self.uav.angle-self.ang2goal)/np.pi) * REW_ANGLE\n",
    "\n",
    "        # FOV obstacle percentage\n",
    "        fov_reward = (np.sum(obs / UAV_FOV_DISTANCE) / UAV_NUM_RAYS) * PEN_OBSTACLE\n",
    "\n",
    "        # Active penalty\n",
    "        act_reward = PEN_THRUST * abs(action[0]) + PEN_ANG * abs(action[1])\n",
    "\n",
    "        return distance_reward, velocity_reward, angle_reward, fov_reward, act_reward, distance_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.uav is not None, \"You forgot to call reset()\"\n",
    "        thrust, angular_momentum = action\n",
    "        # Calculate the thrust direction based on the UAV's angle, note the thrust is implemented via impulse (reverse)\n",
    "        thrust_direction = Box2D.b2Vec2(math.cos(self.uav.angle), math.sin(self.uav.angle))\n",
    "        thrust = np.clip(thrust, 0., 1.)\n",
    "        angular_momentum = np.clip(angular_momentum, -1., 1.) * UAV_ANG_POW\n",
    "        thrust_force = thrust_direction * thrust * UAV_THRUST_POW\n",
    "\n",
    "        # Apply the thrust & angular momentum\n",
    "        self.uav.ApplyLinearImpulse(thrust_force, self.uav.worldCenter, True)\n",
    "        self.uav.ApplyTorque((angular_momentum), True)\n",
    "\n",
    "        # Update the environment state\n",
    "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)  # Advance the Box2D world, Step (float timeStep, int32 velocityIterations, int32 positionIterations)\n",
    "        # Get new observation\n",
    "        new_obs = self._get_obs()\n",
    "        # Calculate reward\n",
    "        distance_reward, velocity_reward, angle_reward, fov_reward, act_reward, distance_reward = self._calculate_reward(new_obs, action)\n",
    "        reward = distance_reward + velocity_reward + angle_reward + fov_reward + act_reward\n",
    "        # Check if episode is done\n",
    "        done = False\n",
    "        if self.game_over:\n",
    "            done = True\n",
    "            reward += PEN_COLLISION\n",
    "        if self.dist2goal <= GOAL_RADIUS:\n",
    "            done = True\n",
    "            reward += REW_GOAL\n",
    "        # Additional info (optional)\n",
    "        raw_reward = np.array((distance_reward/REW_DIST2GOAL,  velocity_reward/REW_VEL, angle_reward/REW_ANGLE, fov_reward/PEN_OBSTACLE, act_reward))\n",
    "        pos = self.uav.position\n",
    "        vel = self.uav.linearVelocity\n",
    "\n",
    "        state = np.array([\n",
    "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
    "            (pos.y - VIEWPORT_H / SCALE / 2) / (VIEWPORT_H / SCALE / 2),\n",
    "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
    "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
    "            self.uav.angle,\n",
    "            20.0 * self.uav.angularVelocity / FPS,\n",
    "            distance_reward/REW_DIST2GOAL,\n",
    "            angle_reward/REW_ANGLE,\n",
    "            *new_obs/UAV_FOV_DISTANCE\n",
    "        ])\n",
    "\n",
    "        return state, reward, done, raw_reward\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to initial state\n",
    "        self._create_walls()\n",
    "        self._create_uav()\n",
    "        self._create_goal()\n",
    "        self._create_obstacles(NUM_OBSTACLES)  \n",
    "        self.game_over = False  \n",
    "        self.world.contactListener = ContactDetector(self)\n",
    "        # Destroy the UAV if it exists\n",
    "        if self.uav:\n",
    "            self.world.DestroyBody(self.uav)\n",
    "\n",
    "        # Destroy existing obstacles\n",
    "        for obstacle in self.obstacles:\n",
    "            if obstacle.body:  # Ensure the body exists before destroying\n",
    "                self.world.DestroyBody(obstacle.body)\n",
    "\n",
    "        # Clear the lists of obstacles and their properties\n",
    "        self.obstacles.clear()\n",
    "        self.obstacles_properties.clear()\n",
    "\n",
    "        # Recreate the UAV and the goal\n",
    "        self._create_uav()\n",
    "        self._create_goal()\n",
    "\n",
    "        # Recreate obstacles\n",
    "        self._create_obstacles(NUM_OBSTACLES)\n",
    "        self.game_over = False\n",
    "\n",
    "        return self.step(np.array([0., 0.]))[0]\n",
    "\n",
    "    def _raycast_distance(self, start_pos, angle, max_distance):\n",
    "        # Calculate the end point of the ray\n",
    "        end_pos = (start_pos.x + max_distance * math.cos(angle), \n",
    "                start_pos.y + max_distance * math.sin(angle))\n",
    "\n",
    "        # Define a callback class to record ray hits\n",
    "        class RayCastCallback(Box2D.b2RayCastCallback):\n",
    "            def __init__(self):\n",
    "                super(RayCastCallback, self).__init__()\n",
    "                self.fixture = None\n",
    "                self.point = None\n",
    "                self.normal = None\n",
    "\n",
    "            def ReportFixture(self, fixture, point, normal, fraction):\n",
    "                self.fixture = fixture\n",
    "                self.point = Box2D.b2Vec2(point)\n",
    "                self.normal = Box2D.b2Vec2(normal)\n",
    "                return fraction  # Returning the fraction leaves the ray cast going to max_distance\n",
    "\n",
    "        # Create a raycast callback instance\n",
    "        callback = RayCastCallback()\n",
    "\n",
    "        # Cast the ray\n",
    "        self.world.RayCast(callback, start_pos, end_pos)\n",
    "\n",
    "        # If a hit was recorded, calculate the distance, else return max_distance\n",
    "        if callback.fixture:\n",
    "            hit_position = callback.point\n",
    "            distance = math.sqrt((hit_position.x - start_pos.x)**2 + (hit_position.y - start_pos.y)**2)\n",
    "            return distance\n",
    "        else:\n",
    "            return max_distance\n",
    "\n",
    "    def _get_obs(self):\n",
    "        fov_array = np.zeros(UAV_NUM_RAYS)\n",
    "\n",
    "        start_angle = self.uav.angle - UAV_FOV / 2\n",
    "        angle_increment = UAV_FOV / UAV_NUM_RAYS\n",
    "\n",
    "        for i in range(UAV_NUM_RAYS):\n",
    "            ray_angle = start_angle + i * angle_increment\n",
    "            distance = self._raycast_distance(self.uav.position, ray_angle, UAV_FOV_DISTANCE)\n",
    "            fov_array[i] = distance\n",
    "\n",
    "        return fov_array\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((VIEWPORT_W, VIEWPORT_H))\n",
    "\n",
    "        # Clear screen\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw the UAV\n",
    "        for body in [self.uav]:  # Currently, we only have the UAV\n",
    "            for fixture in body.fixtures:\n",
    "                shape = fixture.shape\n",
    "                position = body.transform * shape.pos * SCALE\n",
    "                position = (position[0], VIEWPORT_H - position[1])  # Flip Y\n",
    "                pygame.draw.circle(self.screen, (255, 0, 0), [int(x) for x in position], int(shape.radius * SCALE))\n",
    "\n",
    "        # Draw the goal\n",
    "        goal_position = (self.goal[0] * SCALE, VIEWPORT_H - self.goal[1] * SCALE)\n",
    "        pygame.draw.circle(self.screen, (0, 255, 0), [int(x) for x in goal_position], int(GOAL_RADIUS * SCALE))\n",
    "\n",
    "        # Draw the obstacles\n",
    "        for fixture in self.obstacles:\n",
    "            shape = fixture.shape\n",
    "            body = fixture.body\n",
    "\n",
    "            if isinstance(shape, Box2D.b2CircleShape):\n",
    "                # For circle shapes\n",
    "                position = (body.position.x * SCALE, VIEWPORT_H - body.position.y * SCALE)\n",
    "                pygame.draw.circle(self.screen, (0, 0, 255), [int(x) for x in position], int(shape.radius * SCALE))\n",
    "            \n",
    "            elif isinstance(shape, Box2D.b2PolygonShape):\n",
    "                # For polygon shapes (rectangles, triangles)\n",
    "                vertices = [(body.transform * v) * SCALE for v in shape.vertices]\n",
    "                vertices = [(v[0], VIEWPORT_H - v[1]) for v in vertices]\n",
    "                pygame.draw.polygon(self.screen, (0, 0, 255), vertices)\n",
    "\n",
    "        wall_color = (0, 0, 0)  # Black color for walls\n",
    "        for wall in self.walls:\n",
    "            for fixture in wall.fixtures:\n",
    "                shape = fixture.shape\n",
    "                # Since these are edge shapes, they have exactly two vertices\n",
    "                vertex1, vertex2 = shape.vertices\n",
    "                vertex1 = (wall.transform * vertex1) * SCALE\n",
    "                vertex2 = (wall.transform * vertex2) * SCALE\n",
    "                vertex1 = (vertex1[0], VIEWPORT_H - vertex1[1])  # Flip Y\n",
    "                vertex2 = (vertex2[0], VIEWPORT_H - vertex2[1])  # Flip Y\n",
    "                pygame.draw.line(self.screen, wall_color, vertex1, vertex2, int(WALL_THICKNESS * SCALE))\n",
    "\n",
    "        start_angle = self.uav.angle -UAV_FOV / 2\n",
    "        angle_increment = UAV_FOV / UAV_NUM_RAYS\n",
    "        for i in range(UAV_NUM_RAYS):\n",
    "            ray_angle = start_angle + i * angle_increment\n",
    "            distance = self._raycast_distance(self.uav.position, ray_angle, UAV_FOV_DISTANCE)\n",
    "            end_x = self.uav.position.x + distance * math.cos(ray_angle)\n",
    "            end_y = self.uav.position.y + distance * math.sin(ray_angle)\n",
    "            pygame.draw.line(self.screen, (0, 255, 0), (self.uav.position.x * SCALE, VIEWPORT_H - self.uav.position.y * SCALE), (end_x * SCALE, VIEWPORT_H - end_y * SCALE))\n",
    "\n",
    "        if self.uav:\n",
    "            velocity_vector = self.uav.linearVelocity\n",
    "\n",
    "            uav_center = (self.uav.position.x * SCALE, VIEWPORT_H - self.uav.position.y * SCALE)\n",
    "            velocity_end = (uav_center[0] + velocity_vector.x, \n",
    "                            uav_center[1] - velocity_vector.y)  # Subtract y because of Pygame's y-axis direction\n",
    "\n",
    "            pygame.draw.line(self.screen, (0, 0, 0), uav_center, velocity_end, 2) \n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "            self.isopen = False\n",
    "\n",
    "# Example usage\n",
    "#env = SimpleUAVEnv()\n",
    "#env.reset()\n",
    "#env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Initialize the environment\n",
    "env = SimpleUAVEnv()\n",
    "observation = env.reset()\n",
    "\n",
    "# Parameters for the test\n",
    "total_steps = 1  # Total number of steps in the test\n",
    "thrust_increment = 1  # Increment in thrust per step\n",
    "angular_momentum = 0  # Constant angular momentum (for simplicity)\n",
    "\n",
    "# Run the test\n",
    "for step in range(total_steps):\n",
    "    # Gradually increase thrust\n",
    "    thrust = thrust_increment * step\n",
    "\n",
    "    # Create the action (thrust, angular momentum)\n",
    "    action = [thrust, angular_momentum]\n",
    "\n",
    "    # Perform a step in the environment\n",
    "    state, reward, done, raw_reward = env.step(action)\n",
    "\n",
    "    # Render the current state\n",
    "    env.render()\n",
    "\n",
    "    # Break the loop if the episode is done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    # Pause for a short time to see the animation\n",
    "    time.sleep(1/FPS)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9      ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.9104826,  1.       ,  1.       ,\n",
       "        1.       ,  1.       ,  1.       ,  1.       ,  1.       ,\n",
       "        1.       ,  1.       ,  1.       ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.208103360983436 -0.2812272076049491\n"
     ]
    }
   ],
   "source": [
    "print(env.dist2goal, env.ang2goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         0.         0.9104826  1.        -0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(raw_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NN in Jax/Flax**\n",
    "pip install flax (CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef train(model_state, episodes=MAX_EPISODES, steps=1000):\\n    memory = deque(maxlen=2000)\\n    last_100_scores = deque(maxlen=100)\\n    epsilon = 1.0\\n    results = []\\n    acceptable_avg_score_counter = 0\\n\\n    def record_episode(episode_id, step_id, score, results):\\n        nonlocal last_100_scores, acceptable_avg_score_counter\\n\\n        last_100_scores.append(score)\\n        mean_last_100_scores = np.mean(last_100_scores)\\n\\n        if mean_last_100_scores > ACCEPTABLE_AVERAGE_SCORE_THRESHOLD:\\n            acceptable_avg_score_counter += 1\\n        else:\\n            acceptable_avg_score_counter = 0\\n\\n        print(\\n            'Episode:', episode_id,\\n            'Steps:', step_id,\\n            'Score:', score,\\n            '100 Rolling Average:', mean_last_100_scores,\\n            'Acceptable Avg Score Count:', acceptable_avg_score_counter,\\n        )\\n\\n        results.append({\\n            'episode': episode_id,\\n            'steps': step_id,\\n            'score': score,\\n            'rolling_avg_score': mean_last_100_scores\\n        })\\n\\n\\n    for episode_id in range(episodes):\\n        state = env.reset()\\n        score = 0\\n\\n        for step_id in range(steps):\\n            action = choose_action(state, model_state.params, epsilon)\\n            next_state, reward, done, _ = env.step(action)\\n            if done:\\n                next_state = np.zeros(STATE_SIZE)\\n            memory.append((state, action, reward, next_state))\\n            state = next_state if next_state is not None else np.zeros(STATE_SIZE)\\n            score += reward\\n\\n            if len(memory) >= BATCH_SIZE:\\n                batch = random.sample(memory, BATCH_SIZE)\\n                states, actions, rewards, next_states = map(np.array, zip(*batch))\\n                #print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\\n                model_state, _ = train_step(model_state, (states, actions, rewards, next_states), GAMMA)\\n\\n            if done:\\n                record_episode(episode_id, step_id, score, results)\\n                break\\n\\n            epsilon *= DECAY_FACTOR\\n\\n        if acceptable_avg_score_counter >= MAX_ACCEPTABLE_AVG_SCORE_COUNTER:\\n            break\\n        #print('Epsilon:', epsilon)\\n\\n    return model_state, results\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBS_ARR_START_IDX = 8 # The first 8 elements are the UAV's position, velocity, angle, angular velocity, dist2goal, and angle2goal\n",
    "STATE_SIZE = OBS_ARR_START_IDX + UAV_NUM_RAYS\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    input_dim: int\n",
    "    latent_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.input_dim)(x)  \n",
    "        x = nn.silu(x)        \n",
    "        x = nn.Dense(self.latent_dim)(x)   \n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dense(self.latent_dim)(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "#model = ANN()\n",
    "#params =model.init(key, jnp.ones((1, STATE_SIZE)), STATE_SIZE, 64, 2)\n",
    "model = ANN(input_dim=STATE_SIZE, latent_dim=64, output_dim=2)\n",
    "params =model.init(key, jnp.ones((1, STATE_SIZE)))\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Create the training state\n",
    "train_state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "\n",
    "GAMMA = .99\n",
    "DECAY_FACTOR = .99995\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPISODES = 100\n",
    "#MAX_EPISODES = 2\n",
    "ACCEPTABLE_AVERAGE_SCORE_THRESHOLD = 190\n",
    "MAX_ACCEPTABLE_AVG_SCORE_COUNTER = 100\n",
    "\n",
    "# Function to select an action\n",
    "def choose_action(state, params, epsilon):\n",
    "    def random_action():\n",
    "        return np.array([np.random.uniform(0, 1), np.random.uniform(-1, 1)])\n",
    "\n",
    "    def predicted_action():\n",
    "        q_values = model.apply(params, jnp.expand_dims(jnp.array(state), 0))\n",
    "        action = np.argmax(q_values, axis=-1)\n",
    "        return action\n",
    "\n",
    "    return random_action() if np.random.random() <= epsilon else predicted_action()\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, gamma):\n",
    "    def loss_fn(params):\n",
    "        # Extracting experiences from the batch\n",
    "        states, actions, rewards, next_states = batch\n",
    "        q_values = model.apply(params, states)\n",
    "        next_q_values = model.apply(params, next_states)\n",
    "        max_next_q_values = jnp.max(next_q_values, axis=1)\n",
    "        target_q_values = rewards + gamma * max_next_q_values\n",
    "\n",
    "        # Q-value for the action that was taken\n",
    "        actions_one_hot = jax.nn.one_hot(actions, 2)  # Assuming 2 actions\n",
    "        q_action = jnp.sum(q_values * actions_one_hot, axis=1)\n",
    "\n",
    "        # Loss is MSE between q_action and target_q_values\n",
    "        return jnp.mean((target_q_values - q_action) ** 2)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "'''\n",
    "def train(model_state, episodes=MAX_EPISODES, steps=1000):\n",
    "    memory = deque(maxlen=2000)\n",
    "    last_100_scores = deque(maxlen=100)\n",
    "    epsilon = 1.0\n",
    "    results = []\n",
    "    acceptable_avg_score_counter = 0\n",
    "\n",
    "    def record_episode(episode_id, step_id, score, results):\n",
    "        nonlocal last_100_scores, acceptable_avg_score_counter\n",
    "\n",
    "        last_100_scores.append(score)\n",
    "        mean_last_100_scores = np.mean(last_100_scores)\n",
    "\n",
    "        if mean_last_100_scores > ACCEPTABLE_AVERAGE_SCORE_THRESHOLD:\n",
    "            acceptable_avg_score_counter += 1\n",
    "        else:\n",
    "            acceptable_avg_score_counter = 0\n",
    "\n",
    "        print(\n",
    "            'Episode:', episode_id,\n",
    "            'Steps:', step_id,\n",
    "            'Score:', score,\n",
    "            '100 Rolling Average:', mean_last_100_scores,\n",
    "            'Acceptable Avg Score Count:', acceptable_avg_score_counter,\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'episode': episode_id,\n",
    "            'steps': step_id,\n",
    "            'score': score,\n",
    "            'rolling_avg_score': mean_last_100_scores\n",
    "        })\n",
    "\n",
    "\n",
    "    for episode_id in range(episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "\n",
    "        for step_id in range(steps):\n",
    "            action = choose_action(state, model_state.params, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                next_state = np.zeros(STATE_SIZE)\n",
    "            memory.append((state, action, reward, next_state))\n",
    "            state = next_state if next_state is not None else np.zeros(STATE_SIZE)\n",
    "            score += reward\n",
    "\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                batch = random.sample(memory, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states = map(np.array, zip(*batch))\n",
    "                #print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\n",
    "                model_state, _ = train_step(model_state, (states, actions, rewards, next_states), GAMMA)\n",
    "\n",
    "            if done:\n",
    "                record_episode(episode_id, step_id, score, results)\n",
    "                break\n",
    "\n",
    "            epsilon *= DECAY_FACTOR\n",
    "\n",
    "        if acceptable_avg_score_counter >= MAX_ACCEPTABLE_AVG_SCORE_COUNTER:\n",
    "            break\n",
    "        #print('Epsilon:', epsilon)\n",
    "\n",
    "    return model_state, results\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Steps: 0 Score: -99.78322177659737 100 Rolling Average: -99.78322177659737 Acceptable Avg Score Count: 0\n",
      "Episode: 1 Steps: 0 Score: -99.58036442427931 100 Rolling Average: -99.68179310043834 Acceptable Avg Score Count: 0\n",
      "Episode: 2 Steps: 0 Score: -99.2921602751232 100 Rolling Average: -99.55191549199996 Acceptable Avg Score Count: 0\n",
      "Episode: 3 Steps: 0 Score: -99.41187735174036 100 Rolling Average: -99.51690595693506 Acceptable Avg Score Count: 0\n",
      "Episode: 4 Steps: 0 Score: -99.58869705122409 100 Rolling Average: -99.53126417579287 Acceptable Avg Score Count: 0\n",
      "Episode: 5 Steps: 0 Score: -99.43245130109646 100 Rolling Average: -99.51479536334347 Acceptable Avg Score Count: 0\n",
      "Episode: 6 Steps: 0 Score: -98.30110407314451 100 Rolling Average: -99.34141089331504 Acceptable Avg Score Count: 0\n",
      "Episode: 7 Steps: 0 Score: -99.90439416594263 100 Rolling Average: -99.4117838023935 Acceptable Avg Score Count: 0\n",
      "Episode: 8 Steps: 0 Score: -99.6392169815797 100 Rolling Average: -99.43705415563642 Acceptable Avg Score Count: 0\n",
      "Episode: 9 Steps: 0 Score: -99.17083697543869 100 Rolling Average: -99.41043243761663 Acceptable Avg Score Count: 0\n",
      "Episode: 10 Steps: 0 Score: -99.41159526564886 100 Rolling Average: -99.41053814925591 Acceptable Avg Score Count: 0\n",
      "Episode: 11 Steps: 0 Score: -99.32970844887596 100 Rolling Average: -99.40380234089092 Acceptable Avg Score Count: 0\n",
      "Episode: 12 Steps: 0 Score: -99.49920540869093 100 Rolling Average: -99.41114103841399 Acceptable Avg Score Count: 0\n",
      "Episode: 13 Steps: 0 Score: -98.40365015351244 100 Rolling Average: -99.33917740377817 Acceptable Avg Score Count: 0\n",
      "Episode: 14 Steps: 0 Score: -99.40717684162676 100 Rolling Average: -99.34371069963474 Acceptable Avg Score Count: 0\n",
      "Episode: 15 Steps: 0 Score: -100.175487372015 100 Rolling Average: -99.39569674165853 Acceptable Avg Score Count: 0\n",
      "Episode: 16 Steps: 0 Score: -99.8963614640937 100 Rolling Average: -99.42514760768412 Acceptable Avg Score Count: 0\n",
      "Episode: 17 Steps: 0 Score: -99.79773306759914 100 Rolling Average: -99.44584679990163 Acceptable Avg Score Count: 0\n",
      "Episode: 18 Steps: 0 Score: -99.9271025262487 100 Rolling Average: -99.47117604865673 Acceptable Avg Score Count: 0\n",
      "Episode: 19 Steps: 0 Score: -99.31562886243357 100 Rolling Average: -99.46339868934558 Acceptable Avg Score Count: 0\n",
      "Episode: 20 Steps: 0 Score: -99.6015312851131 100 Rolling Average: -99.46997643200118 Acceptable Avg Score Count: 0\n",
      "Episode: 21 Steps: 0 Score: -97.59100161792371 100 Rolling Average: -99.38456848590675 Acceptable Avg Score Count: 0\n",
      "Episode: 22 Steps: 0 Score: -99.54373683221941 100 Rolling Average: -99.39148884878992 Acceptable Avg Score Count: 0\n",
      "Episode: 23 Steps: 0 Score: -97.27178608282671 100 Rolling Average: -99.30316790020811 Acceptable Avg Score Count: 0\n",
      "Episode: 24 Steps: 0 Score: -99.60765888645726 100 Rolling Average: -99.31534753965806 Acceptable Avg Score Count: 0\n",
      "Episode: 25 Steps: 0 Score: -98.82718670232987 100 Rolling Average: -99.29657212283776 Acceptable Avg Score Count: 0\n",
      "Episode: 26 Steps: 0 Score: -99.7234812522983 100 Rolling Average: -99.31238357207704 Acceptable Avg Score Count: 0\n",
      "Episode: 27 Steps: 0 Score: -99.46661431499044 100 Rolling Average: -99.31789181289537 Acceptable Avg Score Count: 0\n",
      "Episode: 28 Steps: 0 Score: -99.63022661360037 100 Rolling Average: -99.32866197843693 Acceptable Avg Score Count: 0\n",
      "Episode: 29 Steps: 0 Score: -98.93025166667978 100 Rolling Average: -99.31538163471168 Acceptable Avg Score Count: 0\n",
      "Episode: 30 Steps: 0 Score: -98.94546730013698 100 Rolling Average: -99.30344891424153 Acceptable Avg Score Count: 0\n",
      "Episode: 31 Steps: 0 Score: -99.51968477921558 100 Rolling Average: -99.31020628502196 Acceptable Avg Score Count: 0\n",
      "Episode: 32 Steps: 0 Score: -99.69050737363129 100 Rolling Average: -99.32173056043436 Acceptable Avg Score Count: 0\n",
      "Episode: 33 Steps: 0 Score: -99.44828452249209 100 Rolling Average: -99.325452735789 Acceptable Avg Score Count: 0\n",
      "Episode: 34 Steps: 0 Score: -99.87223012055009 100 Rolling Average: -99.34107494678217 Acceptable Avg Score Count: 0\n",
      "Episode: 35 Steps: 0 Score: -99.45373574921658 100 Rolling Average: -99.34420441351648 Acceptable Avg Score Count: 0\n",
      "Episode: 36 Steps: 0 Score: -99.68120191437406 100 Rolling Average: -99.3533124540802 Acceptable Avg Score Count: 0\n",
      "Episode: 37 Steps: 0 Score: -99.3062268281085 100 Rolling Average: -99.3520733586599 Acceptable Avg Score Count: 0\n",
      "Episode: 38 Steps: 0 Score: -99.43933491499918 100 Rolling Average: -99.35431083446346 Acceptable Avg Score Count: 0\n",
      "Episode: 39 Steps: 0 Score: -99.64121919001455 100 Rolling Average: -99.36148354335224 Acceptable Avg Score Count: 0\n",
      "Episode: 40 Steps: 0 Score: -99.444610420668 100 Rolling Average: -99.36351102816482 Acceptable Avg Score Count: 0\n",
      "Episode: 41 Steps: 0 Score: -100.14980847730266 100 Rolling Average: -99.38223239600144 Acceptable Avg Score Count: 0\n",
      "Episode: 42 Steps: 0 Score: -98.68764117591319 100 Rolling Average: -99.36607911181333 Acceptable Avg Score Count: 0\n",
      "Episode: 43 Steps: 0 Score: -99.77407461232582 100 Rolling Average: -99.37535173682498 Acceptable Avg Score Count: 0\n",
      "Episode: 44 Steps: 0 Score: -100.00122669946334 100 Rolling Average: -99.38926006932806 Acceptable Avg Score Count: 0\n",
      "Episode: 45 Steps: 0 Score: -99.84652323819489 100 Rolling Average: -99.39920057299908 Acceptable Avg Score Count: 0\n",
      "Episode: 46 Steps: 0 Score: -99.57223570288379 100 Rolling Average: -99.40288217150726 Acceptable Avg Score Count: 0\n",
      "Episode: 47 Steps: 0 Score: -99.5103169500901 100 Rolling Average: -99.40512039606108 Acceptable Avg Score Count: 0\n",
      "Episode: 48 Steps: 0 Score: -99.76555108382301 100 Rolling Average: -99.41247612438275 Acceptable Avg Score Count: 0\n",
      "Episode: 49 Steps: 0 Score: -99.62696048893457 100 Rolling Average: -99.4167658116738 Acceptable Avg Score Count: 0\n",
      "Episode: 50 Steps: 0 Score: -99.96056534615744 100 Rolling Average: -99.42742854764407 Acceptable Avg Score Count: 0\n",
      "Episode: 51 Steps: 0 Score: -97.20245874585767 100 Rolling Average: -99.38464066684047 Acceptable Avg Score Count: 0\n",
      "Episode: 52 Steps: 0 Score: -99.58671858962472 100 Rolling Average: -99.3884534578364 Acceptable Avg Score Count: 0\n",
      "Episode: 53 Steps: 0 Score: -99.3044833358136 100 Rolling Average: -99.38689845557673 Acceptable Avg Score Count: 0\n",
      "Episode: 54 Steps: 0 Score: -99.82090183195947 100 Rolling Average: -99.3947894260564 Acceptable Avg Score Count: 0\n",
      "Episode: 55 Steps: 0 Score: -99.13974369374442 100 Rolling Average: -99.3902350379794 Acceptable Avg Score Count: 0\n",
      "Episode: 56 Steps: 0 Score: -99.76794038820657 100 Rolling Average: -99.3968614476325 Acceptable Avg Score Count: 0\n",
      "Episode: 57 Steps: 0 Score: -99.42865597174645 100 Rolling Average: -99.39740962908274 Acceptable Avg Score Count: 0\n",
      "Episode: 58 Steps: 0 Score: -99.53004318986835 100 Rolling Average: -99.39965765553674 Acceptable Avg Score Count: 0\n",
      "Episode: 59 Steps: 0 Score: -99.48009584914145 100 Rolling Average: -99.40099829209682 Acceptable Avg Score Count: 0\n",
      "Episode: 60 Steps: 0 Score: -99.41412780890887 100 Rolling Average: -99.40121353007734 Acceptable Avg Score Count: 0\n",
      "Episode: 61 Steps: 0 Score: -99.30930492355989 100 Rolling Average: -99.39973113319803 Acceptable Avg Score Count: 0\n",
      "Episode: 62 Steps: 0 Score: -99.60945725177058 100 Rolling Average: -99.40306011920711 Acceptable Avg Score Count: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(64, 2), (64, 2, 2)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/util.py:263\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/util.py:256\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/lax/lax.py:152\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m@cache\u001b[39m()\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_broadcast_shapes_cached\u001b[39m(\u001b[39m*\u001b[39mshapes: \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m--> 152\u001b[0m   \u001b[39mreturn\u001b[39;00m _broadcast_shapes_uncached(\u001b[39m*\u001b[39;49mshapes)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64, 2), (64, 2, 2)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tonyz/myProj/AiProj/uav.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     states, actions, rewards, next_states \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(np\u001b[39m.\u001b[39marray, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m#print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     model_state, _ \u001b[39m=\u001b[39m train_step(model_state, (states, actions, rewards, next_states), GAMMA)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     record_episode(episode_id, step_id, score, results)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "\u001b[1;32m/home/tonyz/myProj/AiProj/uav.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean((target_q_values \u001b[39m-\u001b[39m q_action) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m grad_fn \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvalue_and_grad(loss_fn)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m loss, grads \u001b[39m=\u001b[39m grad_fn(state\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mreturn\u001b[39;00m state\u001b[39m.\u001b[39mapply_gradients(grads\u001b[39m=\u001b[39mgrads), loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "\u001b[1;32m/home/tonyz/myProj/AiProj/uav.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Q-value for the action that was taken\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m actions_one_hot \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mone_hot(actions, \u001b[39m2\u001b[39m)  \u001b[39m# Assuming 2 actions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m q_action \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msum(q_values \u001b[39m*\u001b[39;49m actions_one_hot, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Loss is MSE between q_action and target_q_values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean((target_q_values \u001b[39m-\u001b[39m q_action) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:728\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mop\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 728\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maval, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:256\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    254\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    257\u001b[0m \u001b[39m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(other) \u001b[39min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:96\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(x1, x2, \u001b[39m/\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m   x1, x2 \u001b[39m=\u001b[39m promote_args(numpy_fn\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, x1, x2)\n\u001b[1;32m     97\u001b[0m   \u001b[39mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[39mif\u001b[39;00m x1\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mbool_ \u001b[39melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/util.py:363\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    361\u001b[0m check_arraylike(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[1;32m    362\u001b[0m _check_no_float0s(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m promote_shapes(fun_name, \u001b[39m*\u001b[39;49mpromote_dtypes(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/util.py:248\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mnumpy_rank_promotion\u001b[39m.\u001b[39mvalue \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 248\u001b[0m result_rank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(lax\u001b[39m.\u001b[39;49mbroadcast_shapes(\u001b[39m*\u001b[39;49mshapes))\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (result_rank \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(shp)) \u001b[39m+\u001b[39m shp)\n\u001b[1;32m    250\u001b[0m         \u001b[39mfor\u001b[39;00m arg, shp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    166\u001b[0m result_shape \u001b[39m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64, 2), (64, 2, 2)]"
     ]
    }
   ],
   "source": [
    "episodes =100\n",
    "steps = 1000\n",
    "memory = deque(maxlen=2000)\n",
    "last_100_scores = deque(maxlen=100)\n",
    "epsilon = 1.0\n",
    "results = []\n",
    "acceptable_avg_score_counter = 0\n",
    "model_state = train_state\n",
    "\n",
    "def record_episode(episode_id, step_id, score, results):\n",
    "    last_100_scores.append(score)\n",
    "    mean_last_100_scores = np.mean(last_100_scores)\n",
    "\n",
    "    if mean_last_100_scores > ACCEPTABLE_AVERAGE_SCORE_THRESHOLD:\n",
    "        acceptable_avg_score_counter += 1\n",
    "    else:\n",
    "        acceptable_avg_score_counter = 0\n",
    "\n",
    "    print(\n",
    "        'Episode:', episode_id,\n",
    "        'Steps:', step_id,\n",
    "        'Score:', score,\n",
    "        '100 Rolling Average:', mean_last_100_scores,\n",
    "        'Acceptable Avg Score Count:', acceptable_avg_score_counter,\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        'episode': episode_id,\n",
    "        'steps': step_id,\n",
    "        'score': score,\n",
    "        'rolling_avg_score': mean_last_100_scores\n",
    "    })\n",
    "\n",
    "\n",
    "for episode_id in range(episodes):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "\n",
    "    for step_id in range(steps):\n",
    "        action = choose_action(state, model_state.params, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            next_state = np.zeros(STATE_SIZE)\n",
    "        memory.append((state, action, reward, next_state))\n",
    "        state = next_state if next_state is not None else np.zeros(STATE_SIZE)\n",
    "        score += reward\n",
    "\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            batch = random.sample(memory, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states = map(np.array, zip(*batch))\n",
    "            #print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\n",
    "            model_state, _ = train_step(model_state, (states, actions, rewards, next_states), GAMMA)\n",
    "\n",
    "        if done:\n",
    "            record_episode(episode_id, step_id, score, results)\n",
    "            break\n",
    "\n",
    "        epsilon *= DECAY_FACTOR\n",
    "\n",
    "    if acceptable_avg_score_counter >= MAX_ACCEPTABLE_AVG_SCORE_COUNTER:\n",
    "        break\n",
    "    #print('Epsilon:', epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 18) (64, 2) (64,) (64, 18)\n"
     ]
    }
   ],
   "source": [
    "print(states.shape, actions.shape, rewards.shape, next_states.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
