{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Optional\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import error, spaces\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from gymnasium.utils import EzPickle, colorize\n",
    "from gymnasium.utils.step_api_compatibility import step_api_compatibility\n",
    "\n",
    "import pygame\n",
    "\n",
    "import Box2D\n",
    "from Box2D.b2 import (\n",
    "    circleShape,\n",
    "    contactListener,\n",
    "    edgeShape,\n",
    "    fixtureDef,\n",
    "    polygonShape,\n",
    "    revoluteJointDef,\n",
    "    staticBody, \n",
    "    dynamicBody\n",
    ")\n",
    "\n",
    "import pygame\n",
    "\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "#from flax import linen as nn\n",
    "#from flax.training import train_state\n",
    "#import optax\n",
    "from collections import deque\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **state Space**\n",
    "$(x, y, p_x, p_y, angle, L, mass, intertia)$ : most simplified\n",
    "\n",
    "### **observation**\n",
    "(FOV array)\n",
    "\n",
    "### **Action space**\n",
    "(Propulsion (acc in facing angle), angular velocity)\n",
    "```python\n",
    "thrust, angular_momentum = action\n",
    "thrust = np.clip(thrust, 0., 1.)\n",
    "angular_momentum = np.clip(angular_momentum, -1., 1.) * UAV_ANG_POW\n",
    "thrust_force = thrust_direction * thrust * UAV_THRUST_POW\n",
    "```\n",
    "Note the input of thrust and angular momentum needs to be interpretated as 0-100%, -100%-100%\n",
    "\n",
    "### **Rewards**\n",
    "*   increase/decrease the closer/durther the agent is to the goal.\n",
    "*   increase/decrease the larger/smaller the magnitute of velocity $||v||$.\n",
    "*   increase/decrease the larger/smaller the $\\%$ of obstacle in FOV.\n",
    "\n",
    "The episode receive an additional reward of -100 for crashing.\n",
    "\n",
    "An episode is considered a solution if it reaches score.\n",
    "\n",
    "### **Starting State**\n",
    "The agent starts at the left ceter of the viewpoirt, with a initial $v_x$\n",
    "\n",
    "### **Episode Termination**\n",
    "If:\n",
    "* The agent crashes\n",
    "* The agent gets outside of the viewport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Constants\n",
    "FPS = 50\n",
    "SCALE = 10.0  # Scaling for visualization\n",
    "OBSTACLE_SCALE = 5.0  # Scaling for obstacles\n",
    "UAV_RADIUS = 10.0 / SCALE  # Radius of the UAV\n",
    "GOAL_RADIUS = UAV_RADIUS  # Radius of the Goal\n",
    "VIEWPORT_W = 600\n",
    "VIEWPORT_H = 400\n",
    "WALL_THICKNESS = 1.0  # Thickness of the walls\n",
    "NUM_OBSTACLES = 10  # Number of obstacles\n",
    "OBS_MAX_RAD = min(VIEWPORT_W, VIEWPORT_H) / SCALE / OBSTACLE_SCALE # Maximum radius of obstacles\n",
    "OBS_MIN_RAD = OBS_MAX_RAD / 3  # Minimum radius of obstacles\n",
    "MIN_CLEARANCE = UAV_RADIUS * 3  # Minimum clearance between obstacles\n",
    "\n",
    "#Note: For the angle in box2d, 0 rad is at 3 o'clock, and positive angle is in the clockwise direction\n",
    "\n",
    "# UAV specs\n",
    "UAV_INI_ANGLE = np.deg2rad(0)\n",
    "UAV_DENSITY = 1.0\n",
    "UAV_FRICTION = 0.3\n",
    "UAV_FOV = np.deg2rad(90)  # Field of View in degrees\n",
    "UAV_NUM_RAYS = 10  # Number of rays in the FOV array\n",
    "UAV_FOV_DISTANCE = 100 / SCALE  # Maximum sensing distance\n",
    "UAV_ANG_POW = 10  # Maximum angular velocity (deg/s?)\n",
    "UAV_THRUST_POW = 10  # Maximum thrust (Unit/s?)\n",
    "\n",
    "# Penalty/Reward coeff\n",
    "PEN_THRUST = -0.01\n",
    "PEN_ANG = -0.01\n",
    "PEN_OBSTACLE = -2\n",
    "PEN_COLLISION = -100\n",
    "REW_VEL = 0.5\n",
    "REW_ANGLE = 0.05\n",
    "REW_GOAL = 100\n",
    "REW_DIST2GOAL = 1\n",
    "\n",
    "class ContactDetector(Box2D.b2ContactListener):\n",
    "    def __init__(self, env):\n",
    "        super(ContactDetector, self).__init__()\n",
    "        self.env = env\n",
    "\n",
    "    def BeginContact(self, contact):\n",
    "        # Check if one of the bodies is the UAV and the other is an obstacle or wall\n",
    "        if contact.fixtureA.body == self.env.uav or contact.fixtureB.body == self.env.uav:\n",
    "            self.env.game_over = True\n",
    "\n",
    "class SimpleUAVEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': FPS}\n",
    "\n",
    "    def __init__(self):\n",
    "        # Define the Box2D world\n",
    "        self.world = Box2D.b2World(gravity=(0, 0))\n",
    "        self.uav = None  # UAV object\n",
    "        self.goal = None  # Goal position\n",
    "        self.dist2goal = None  # Distance to goal\n",
    "        self.walls = []  # Walls\n",
    "        self.obstacles = []  # Store properties of obstacles\n",
    "        self.obstacles_properties = []\n",
    "\n",
    "        # For rendering\n",
    "        self.screen = None\n",
    "        self.isopen = True\n",
    "\n",
    "    def _create_walls(self):\n",
    "        # Create walls around the viewport\n",
    "        wall_shapes = [\n",
    "            edgeShape(vertices=[(0, 0), (VIEWPORT_W/SCALE, 0)]),  # Bottom\n",
    "            edgeShape(vertices=[(0, 0), (0, VIEWPORT_H/SCALE)]),  # Left\n",
    "            edgeShape(vertices=[(0, VIEWPORT_H/SCALE), (VIEWPORT_W/SCALE, VIEWPORT_H/SCALE)]),  # Top\n",
    "            edgeShape(vertices=[(VIEWPORT_W/SCALE, 0), (VIEWPORT_W/SCALE, VIEWPORT_H/SCALE)])  # Right\n",
    "        ]\n",
    "        for shape in wall_shapes:\n",
    "            wall_body = self.world.CreateStaticBody(\n",
    "                position=(0, 0),\n",
    "                shapes=shape\n",
    "            )\n",
    "            self.walls.append(wall_body)\n",
    "\n",
    "    def _is_position_valid(self, new_properties):\n",
    "        # Check against existing obstacles\n",
    "        for prop in self.obstacles_properties:\n",
    "            distance = math.sqrt((new_properties['centroid_x'] - prop['centroid_x'])**2 + \n",
    "                                (new_properties['centroid_y'] - prop['centroid_y'])**2)\n",
    "            if distance < (new_properties['max_span'] + prop['max_span'] + MIN_CLEARANCE):\n",
    "                return False\n",
    "\n",
    "        # Check distance to the goal\n",
    "        goal_distance = math.sqrt((new_properties['centroid_x'] - self.goal[0])**2 +\n",
    "                                (new_properties['centroid_y'] - self.goal[1])**2)\n",
    "        if goal_distance < (new_properties['max_span'] + 1.5*UAV_RADIUS):\n",
    "            return False\n",
    "\n",
    "        # Check distance to the uav starting position\n",
    "        uav_distance = math.sqrt((new_properties['centroid_x'] - self.uav_start_pos[0])**2 +\n",
    "                                (new_properties['centroid_y'] - self.uav_start_pos[1])**2)\n",
    "        if uav_distance < (new_properties['max_span'] + 1.5*UAV_RADIUS):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def _generate_triangle_properties(self):\n",
    "        # Randomly generate centroid within bounds\n",
    "        centroid_x = random.uniform(UAV_RADIUS, VIEWPORT_W / SCALE - UAV_RADIUS)\n",
    "        centroid_y = random.uniform(UAV_RADIUS, VIEWPORT_H / SCALE - UAV_RADIUS)\n",
    "        \n",
    "        # Random length from centroid to vertices\n",
    "        length = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "\n",
    "        # Angle offsets for equilateral triangle vertices\n",
    "        angle_offset = math.pi * 2 / 3\n",
    "\n",
    "        # Calculate vertices based on centroid and length\n",
    "        vertices = []\n",
    "        for i in range(3):\n",
    "            angle = angle_offset * i\n",
    "            vertex_x = centroid_x + length * math.cos(angle)\n",
    "            vertex_y = centroid_y + length * math.sin(angle)\n",
    "            vertices.append((vertex_x, vertex_y))\n",
    "\n",
    "        # Random rotation angle\n",
    "        rotation_angle = random.uniform(0, math.pi)\n",
    "\n",
    "        return {'type': 'triangle', 'vertices': vertices, 'centroid_x': centroid_x, 'centroid_y': centroid_y, 'angle': rotation_angle, 'max_span': length}\n",
    "\n",
    "    def _generate_rectangle_properties(self):\n",
    "        width = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "        height = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "        angle = random.uniform(0, math.pi)  # Random angle in radians\n",
    "\n",
    "        centroid_x = random.uniform(width / 2, VIEWPORT_W / SCALE - width / 2)\n",
    "        centroid_y = random.uniform(height / 2, VIEWPORT_H / SCALE - height / 2)\n",
    "\n",
    "        return {'type': 'rectangle', 'centroid_x': centroid_x, 'centroid_y': centroid_y, 'width': width, 'height': height, 'angle': angle, 'max_span': max(width, height)}\n",
    "\n",
    "    def _generate_circle_properties(self):\n",
    "        radius = random.uniform(OBS_MIN_RAD, OBS_MAX_RAD)\n",
    "        centroid_x = random.uniform(radius, VIEWPORT_W / SCALE - radius)\n",
    "        centroid_y = random.uniform(radius, VIEWPORT_H / SCALE - radius)\n",
    "\n",
    "        return {'type': 'circle', 'centroid_x': centroid_x, 'centroid_y': centroid_y, 'max_span': radius}\n",
    "\n",
    "    def _create_obstacle_from_properties(self, properties):\n",
    "        if properties['type'] == 'circle':\n",
    "            body = self.world.CreateStaticBody(position=(properties['centroid_x'], properties['centroid_y']))\n",
    "            circle = body.CreateCircleFixture(radius=properties['max_span'], density=1, friction=0.3)\n",
    "            self.obstacles.append(circle)\n",
    "\n",
    "        elif properties['type'] == 'rectangle':\n",
    "            body = self.world.CreateStaticBody(position=(properties['centroid_x'], properties['centroid_y']))\n",
    "            rectangle = body.CreatePolygonFixture(box=(properties['width'] / 2, properties['height'] / 2), density=1, friction=0.3)\n",
    "            body.angle = properties['angle']\n",
    "            self.obstacles.append(rectangle)\n",
    "\n",
    "        elif properties['type'] == 'triangle':\n",
    "            vertices = [(v[0] - properties['centroid_x'], v[1] - properties['centroid_y']) for v in properties['vertices']]\n",
    "            body = self.world.CreateStaticBody(position=(properties['centroid_x'], properties['centroid_y']))\n",
    "            triangle = body.CreatePolygonFixture(vertices=vertices, density=1, friction=0.3)\n",
    "            body.angle = properties['angle']\n",
    "            self.obstacles.append(triangle)\n",
    "\n",
    "    def _create_obstacles(self, num_obstacles=5):\n",
    "        num_obstacles = num_obstacles\n",
    "        obstacle_types = ['triangle', 'rectangle', 'circle']\n",
    "        max_iter = 1000\n",
    "        for _ in range(num_obstacles):\n",
    "            obstacle_type = random.choice(obstacle_types)\n",
    "            for iter in range(max_iter):\n",
    "                if obstacle_type == 'triangle':\n",
    "                    properties = self._generate_triangle_properties()\n",
    "                elif obstacle_type == 'rectangle':\n",
    "                    properties = self._generate_rectangle_properties()\n",
    "                elif obstacle_type == 'circle':\n",
    "                    properties = self._generate_circle_properties()\n",
    "\n",
    "                # Check if the new obstacle overlaps with existing ones\n",
    "                if self._is_position_valid(properties):\n",
    "                    self.obstacles_properties.append(properties)\n",
    "                    # Create the actual obstacle based on properties\n",
    "                    #print(properties['type']+': pass checking')\n",
    "                    self._create_obstacle_from_properties(properties)\n",
    "                    break\n",
    "                #if iter == max_iter - 1:\n",
    "                #    print('Failed to create: ', obstacle_type)\n",
    "\n",
    "    def _create_uav(self):\n",
    "        # Create the UAV at a position away from the left wall\n",
    "        uav_start_pos = (UAV_RADIUS + 2 * WALL_THICKNESS, VIEWPORT_H / SCALE / 2)\n",
    "        self.uav_start_pos = uav_start_pos\n",
    "        self.uav = self.world.CreateDynamicBody(position=uav_start_pos, angle=UAV_INI_ANGLE, linearVelocity=(0,0), angularVelocity=0.0)\n",
    "        self.uav.CreateCircleFixture(radius=UAV_RADIUS, density=UAV_DENSITY, friction=UAV_FRICTION)\n",
    "\n",
    "    def _create_goal(self):\n",
    "        # Create a random goal position away from the right wall\n",
    "        goal_pos_x = VIEWPORT_W / SCALE - GOAL_RADIUS - 2 * WALL_THICKNESS\n",
    "        goal_pos_y = random.uniform(WALL_THICKNESS+UAV_RADIUS, VIEWPORT_H / SCALE - UAV_RADIUS)\n",
    "        self.goal = (goal_pos_x, goal_pos_y)\n",
    "        self.ini_to_goal_dist = math.sqrt((self.uav_start_pos[0] - self.goal[0])**2 + (self.uav_start_pos[1] - self.goal[1])**2)\n",
    "\n",
    "    def _calculate_reward(self, obs, action):\n",
    "        # Distance to goal\n",
    "        dist2goal = math.sqrt((self.uav.position.x - self.goal[0])**2 + (self.uav.position.y - self.goal[1])**2)\n",
    "        self.dist2goal = dist2goal\n",
    "        distance_reward = (1 - dist2goal / self.ini_to_goal_dist) * REW_DIST2GOAL  # Normalize \n",
    "\n",
    "        # Velocity reward\n",
    "        velocity_reward = self.uav.linearVelocity.length * REW_VEL\n",
    "\n",
    "        # Angle reward\n",
    "        self.ang2goal = (np.arctan2(self.goal[1] - self.uav.position.y, self.goal[0] - self.uav.position.x) - self.uav.angle + np.pi) % (2 * np.pi) - np.pi\n",
    "        angle_reward = (1 - abs(self.uav.angle-self.ang2goal)/np.pi) * REW_ANGLE\n",
    "\n",
    "        # FOV obstacle percentage\n",
    "        fov_reward = (np.sum(obs / UAV_FOV_DISTANCE) / UAV_NUM_RAYS) * PEN_OBSTACLE\n",
    "\n",
    "        # Active penalty\n",
    "        act_reward = PEN_THRUST * abs(action[0]) + PEN_ANG * abs(action[1])\n",
    "\n",
    "        return distance_reward, velocity_reward, angle_reward, fov_reward, act_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.uav is not None, \"You forgot to call reset()\"\n",
    "        action = action.flatten()\n",
    "        thrust = action[0]\n",
    "        angular_momentum = action[1]\n",
    "        # Calculate the thrust direction based on the UAV's angle, note the thrust is implemented via impulse (reverse)\n",
    "        thrust_direction = Box2D.b2Vec2(math.cos(self.uav.angle), math.sin(self.uav.angle))\n",
    "        thrust = np.clip(thrust, 0., 1.)\n",
    "        angular_momentum = np.clip(angular_momentum, -1., 1.) * UAV_ANG_POW\n",
    "        thrust_force = thrust_direction * thrust * UAV_THRUST_POW\n",
    "\n",
    "        # Apply the thrust & angular momentum\n",
    "        self.uav.ApplyLinearImpulse(thrust_force, self.uav.worldCenter, True)\n",
    "        self.uav.ApplyTorque((angular_momentum), True)\n",
    "\n",
    "        # Update the environment state\n",
    "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)  # Advance the Box2D world, Step (float timeStep, int32 velocityIterations, int32 positionIterations)\n",
    "        # Get new observation\n",
    "        new_obs = self._get_obs()\n",
    "        # Calculate reward\n",
    "        distance_reward, velocity_reward, angle_reward, fov_reward, act_reward = self._calculate_reward(new_obs, action)\n",
    "        reward = distance_reward + velocity_reward + angle_reward + fov_reward + act_reward\n",
    "        # Check if episode is done\n",
    "        done = False\n",
    "        if self.game_over:\n",
    "            done = True\n",
    "            reward += PEN_COLLISION\n",
    "        if self.dist2goal <= GOAL_RADIUS:\n",
    "            done = True\n",
    "            reward += REW_GOAL\n",
    "        # Additional info (optional)\n",
    "        raw_reward = np.array((distance_reward/REW_DIST2GOAL,  velocity_reward/REW_VEL, angle_reward/REW_ANGLE, fov_reward/PEN_OBSTACLE, act_reward))\n",
    "        pos = self.uav.position\n",
    "        vel = self.uav.linearVelocity\n",
    "\n",
    "        state = np.array([\n",
    "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
    "            (pos.y - VIEWPORT_H / SCALE / 2) / (VIEWPORT_H / SCALE / 2),\n",
    "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
    "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
    "            self.uav.angle,\n",
    "            20.0 * self.uav.angularVelocity / FPS,\n",
    "            distance_reward/REW_DIST2GOAL,\n",
    "            angle_reward/REW_ANGLE,\n",
    "            *new_obs/UAV_FOV_DISTANCE\n",
    "        ])\n",
    "\n",
    "        return state, reward, done, raw_reward\n",
    "\n",
    "    def _destroy(self):\n",
    "        if not self.uav:\n",
    "            return\n",
    "        self.world.contactListener = None\n",
    "        self.world.DestroyBody(self.uav)\n",
    "        self.uav = None\n",
    "        for obstacle in self.obstacles:\n",
    "            self.world.DestroyBody(obstacle.body)\n",
    "        self.obstacles = []  # Store properties of obstacles\n",
    "        self.obstacles_properties = []\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to initial state\n",
    "        self._destroy()\n",
    "        self.world.contactListener_keepref = ContactDetector(self)\n",
    "        self.world.contactListener = self.world.contactListener_keepref\n",
    "\n",
    "        self._create_walls()\n",
    "        self._create_uav()\n",
    "        self._create_goal()\n",
    "        self._create_obstacles(NUM_OBSTACLES)\n",
    "        self.game_over = False\n",
    "\n",
    "        return self.step(np.array([0., 0.]))[0]\n",
    "\n",
    "    def _raycast_distance(self, start_pos, angle, max_distance):\n",
    "        # Calculate the end point of the ray\n",
    "        end_pos = (start_pos.x + max_distance * math.cos(angle), \n",
    "                start_pos.y + max_distance * math.sin(angle))\n",
    "\n",
    "        # Define a callback class to record ray hits\n",
    "        class RayCastCallback(Box2D.b2RayCastCallback):\n",
    "            def __init__(self):\n",
    "                super(RayCastCallback, self).__init__()\n",
    "                self.fixture = None\n",
    "                self.point = None\n",
    "                self.normal = None\n",
    "\n",
    "            def ReportFixture(self, fixture, point, normal, fraction):\n",
    "                self.fixture = fixture\n",
    "                self.point = Box2D.b2Vec2(point)\n",
    "                self.normal = Box2D.b2Vec2(normal)\n",
    "                return fraction  # Returning the fraction leaves the ray cast going to max_distance\n",
    "\n",
    "        # Create a raycast callback instance\n",
    "        callback = RayCastCallback()\n",
    "\n",
    "        # Cast the ray\n",
    "        self.world.RayCast(callback, start_pos, end_pos)\n",
    "\n",
    "        # If a hit was recorded, calculate the distance, else return max_distance\n",
    "        if callback.fixture:\n",
    "            hit_position = callback.point\n",
    "            distance = math.sqrt((hit_position.x - start_pos.x)**2 + (hit_position.y - start_pos.y)**2)\n",
    "            return distance\n",
    "        else:\n",
    "            return max_distance\n",
    "\n",
    "    def _get_obs(self):\n",
    "        fov_array = np.zeros(UAV_NUM_RAYS)\n",
    "\n",
    "        start_angle = self.uav.angle - UAV_FOV / 2\n",
    "        angle_increment = UAV_FOV / UAV_NUM_RAYS\n",
    "\n",
    "        for i in range(UAV_NUM_RAYS):\n",
    "            ray_angle = start_angle + i * angle_increment\n",
    "            distance = self._raycast_distance(self.uav.position, ray_angle, UAV_FOV_DISTANCE)\n",
    "            fov_array[i] = distance\n",
    "\n",
    "        return fov_array\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((VIEWPORT_W, VIEWPORT_H))\n",
    "\n",
    "        # Clear screen\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw the UAV\n",
    "        for body in [self.uav]:  # Currently, we only have the UAV\n",
    "            for fixture in body.fixtures:\n",
    "                shape = fixture.shape\n",
    "                position = body.transform * shape.pos * SCALE\n",
    "                position = (position[0], VIEWPORT_H - position[1])  # Flip Y\n",
    "                pygame.draw.circle(self.screen, (255, 0, 0), [int(x) for x in position], int(shape.radius * SCALE))\n",
    "\n",
    "        # Draw the goal\n",
    "        goal_position = (self.goal[0] * SCALE, VIEWPORT_H - self.goal[1] * SCALE)\n",
    "        pygame.draw.circle(self.screen, (0, 255, 0), [int(x) for x in goal_position], int(GOAL_RADIUS * SCALE))\n",
    "\n",
    "        # Draw the obstacles\n",
    "        for fixture in self.obstacles:\n",
    "            shape = fixture.shape\n",
    "            body = fixture.body\n",
    "\n",
    "            if isinstance(shape, Box2D.b2CircleShape):\n",
    "                # For circle shapes\n",
    "                position = (body.position.x * SCALE, VIEWPORT_H - body.position.y * SCALE)\n",
    "                pygame.draw.circle(self.screen, (0, 0, 255), [int(x) for x in position], int(shape.radius * SCALE))\n",
    "            \n",
    "            elif isinstance(shape, Box2D.b2PolygonShape):\n",
    "                # For polygon shapes (rectangles, triangles)\n",
    "                vertices = [(body.transform * v) * SCALE for v in shape.vertices]\n",
    "                vertices = [(v[0], VIEWPORT_H - v[1]) for v in vertices]\n",
    "                pygame.draw.polygon(self.screen, (0, 0, 255), vertices)\n",
    "\n",
    "        wall_color = (0, 0, 0)  # Black color for walls\n",
    "        for wall in self.walls:\n",
    "            for fixture in wall.fixtures:\n",
    "                shape = fixture.shape\n",
    "                # Since these are edge shapes, they have exactly two vertices\n",
    "                vertex1, vertex2 = shape.vertices\n",
    "                vertex1 = (wall.transform * vertex1) * SCALE\n",
    "                vertex2 = (wall.transform * vertex2) * SCALE\n",
    "                vertex1 = (vertex1[0], VIEWPORT_H - vertex1[1])  # Flip Y\n",
    "                vertex2 = (vertex2[0], VIEWPORT_H - vertex2[1])  # Flip Y\n",
    "                pygame.draw.line(self.screen, wall_color, vertex1, vertex2, int(WALL_THICKNESS * SCALE))\n",
    "\n",
    "        start_angle = self.uav.angle -UAV_FOV / 2\n",
    "        angle_increment = UAV_FOV / UAV_NUM_RAYS\n",
    "        for i in range(UAV_NUM_RAYS):\n",
    "            ray_angle = start_angle + i * angle_increment\n",
    "            distance = self._raycast_distance(self.uav.position, ray_angle, UAV_FOV_DISTANCE)\n",
    "            end_x = self.uav.position.x + distance * math.cos(ray_angle)\n",
    "            end_y = self.uav.position.y + distance * math.sin(ray_angle)\n",
    "            pygame.draw.line(self.screen, (0, 255, 0), (self.uav.position.x * SCALE, VIEWPORT_H - self.uav.position.y * SCALE), (end_x * SCALE, VIEWPORT_H - end_y * SCALE))\n",
    "\n",
    "        if self.uav:\n",
    "            velocity_vector = self.uav.linearVelocity\n",
    "\n",
    "            uav_center = (self.uav.position.x * SCALE, VIEWPORT_H - self.uav.position.y * SCALE)\n",
    "            velocity_end = (uav_center[0] + velocity_vector.x, \n",
    "                            uav_center[1] - velocity_vector.y)  # Subtract y because of Pygame's y-axis direction\n",
    "\n",
    "            pygame.draw.line(self.screen, (0, 0, 0), uav_center, velocity_end, 2) \n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reset\n",
    "env = SimpleUAVEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.93935139e-01  2.99744091e+01  9.06858344e-01  3.82425277e-01\n",
      " -1.00000000e-02] -85.44836794555096\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Initialize the environment\n",
    "observation = env.reset()\n",
    "\n",
    "# Parameters for the test\n",
    "total_steps = 100  # Total number of steps in the test\n",
    "thrust_increment = 1  # Increment in thrust per step\n",
    "angular_momentum = 0  # Constant angular momentum (for simplicity)\n",
    "\n",
    "# Run the test\n",
    "for step in range(total_steps):\n",
    "    # Gradually increase thrust\n",
    "    thrust = thrust_increment \n",
    "\n",
    "    # Create the action (thrust, angular momentum)\n",
    "    action = np.array([thrust, angular_momentum])\n",
    "\n",
    "    # Perform a step in the environment\n",
    "    state, reward, done, raw_reward = env.step(action)\n",
    "\n",
    "    # Render the current state\n",
    "    env.render()\n",
    "\n",
    "    # Break the loop if the episode is done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    # Pause for a short time to see the animation\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(raw_reward, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.01903938,  0.9968334 ,  1.00789235,  1.05465081,  1.14878649,\n",
       "        1.32257473,  1.69275053, 10.        , 10.        , 10.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.70547040e-01,  5.17749786e-03,  1.15687683e+01,  9.17995605e+00,\n",
       "       -1.27823174e-01, -1.20235443e+01,  2.93935139e-01,  9.06858344e-01,\n",
       "        1.01903938e-01,  9.96833397e-02,  1.00789235e-01,  1.05465081e-01,\n",
       "        1.14878649e-01,  1.32257473e-01,  1.69275053e-01,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.14246992222265 0.16478996930306433\n"
     ]
    }
   ],
   "source": [
    "print(env.dist2goal, env.ang2goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.93935139e-01  2.99744091e+01  9.06858344e-01  3.82425277e-01\n",
      " -1.00000000e-02] -85.44836794555096\n"
     ]
    }
   ],
   "source": [
    "print(raw_reward, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    '''\n",
    "    This class defines a deep Q-network (DQN), a type of artificial neural network used in reinforcement learning.\n",
    "    The DQN is used to estimate the Q-values, which represent the expected return for each action in each state.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state_size: int, default=8\n",
    "        The size of the state space.\n",
    "    action_size: int, default=4\n",
    "        The size of the action space.\n",
    "    hidden_size: int, default=64\n",
    "        The size of the hidden layers in the network.\n",
    "    '''\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
    "        '''\n",
    "        Initialize a network with the following architecture:\n",
    "            Input layer (state_size, hidden_size)\n",
    "            Hidden layer 1 (hidden_size, hidden_size)\n",
    "            Output layer (hidden_size, action_size)\n",
    "        '''\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Define the forward pass of the DQN. This function is called when the network is called to estimate Q-values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state: torch.Tensor\n",
    "            The state for which to estimate the Q-values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The estimated Q-values for each action in the input state.\n",
    "        '''\n",
    "        x = torch.relu(self.layer1(state))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    '''\n",
    "    This class represents a replay buffer, a type of data structure commonly used in reinforcement learning algorithms.\n",
    "    The buffer stores past experiences in the environment, allowing the agent to sample and learn from them at later times.\n",
    "    This helps to break the correlation of sequential observations and stabilize the learning process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    buffer_size: int, default=10000\n",
    "        The maximum number of experiences that can be stored in the buffer.\n",
    "    '''\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Add a new experience to the buffer. Each experience is a tuple containing a state, action, reward,\n",
    "        the resulting next state, and a done flag indicating whether the episode has ended.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array-like\n",
    "            The state of the environment before taking the action.\n",
    "        action: int\n",
    "            The action taken by the agent.\n",
    "        reward: float\n",
    "            The reward received after taking the action.\n",
    "        next_state: array-like\n",
    "            The state of the environment after taking the action.\n",
    "        done: bool\n",
    "            A flag indicating whether the episode has ended after taking the action.\n",
    "        '''\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Randomly sample a batch of experiences from the buffer. The batch size must be smaller or equal to the current number of experiences in the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            The number of experiences to sample from the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of numpy.ndarray\n",
    "            A tuple containing arrays of states, actions, rewards, next states, and done flags.\n",
    "        '''\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Get the current number of experiences in the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of experiences in the buffer.\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    '''\n",
    "    This class represents a Deep Q-Learning agent that uses a Deep Q-Network (DQN) and a replay memory to interact \n",
    "    with its environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_size: int, default=8\n",
    "        The size of the state space.\n",
    "    action_size: int, default=4\n",
    "        The size of the action space.\n",
    "    hidden_size: int, default=64\n",
    "        The size of the hidden layers in the network.\n",
    "    learning_rate: float, default=1e-3\n",
    "        The learning rate for the optimizer.\n",
    "    gamma: float, default=0.99\n",
    "        The discount factor for future rewards.\n",
    "    buffer_size: int, default=10000\n",
    "        The maximum size of the replay memory.\n",
    "    batch_size: int, default=64\n",
    "        The batch size for learning from the replay memory.\n",
    "    '''\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64, \n",
    "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
    "        # Select device to train on (if CUDA available, use it, otherwise use CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Discount factor for future rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Batch size for sampling from the replay memory\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Number of possible actions\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Initialize the Q-Network and Target Network with the given state size, action size and hidden layer size\n",
    "        # Move the networks to the selected device\n",
    "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        \n",
    "        # Set weights of target network to be the same as those of the q network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Set target network to evaluation mode\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Initialize the optimizer for updating the Q-Network's parameters\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Initialize the replay memory\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Perform a step in the environment, store the experience in the replay memory and potentially update the Q-network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array-like\n",
    "            The current state of the environment.\n",
    "        action: int\n",
    "            The action taken by the agent.\n",
    "        reward: float\n",
    "            The reward received after taking the action.\n",
    "        next_state: array-like\n",
    "            The state of the environment after taking the action.\n",
    "        done: bool\n",
    "            A flag indicating whether the episode has ended after taking the action.\n",
    "        '''\n",
    "        # Store the experience in memory\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # If there are enough experiences in memory, perform a learning step\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        '''\n",
    "        Choose an action based on the current state and the epsilon-greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array-like\n",
    "            The current state of the environment.\n",
    "        eps: float, default=0.\n",
    "            The epsilon for the epsilon-greedy policy. With probability eps, a random action is chosen.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The chosen action.\n",
    "        '''\n",
    "        # If a randomly chosen value is greater than eps\n",
    "        if random.random() > eps:  \n",
    "            # Convert state to a PyTorch tensor and set network to evaluation mode\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)  \n",
    "            self.q_network.eval()  \n",
    "\n",
    "            # With no gradient updates, get the action values from the DQN\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "\n",
    "            # Revert to training mode and return action\n",
    "            self.q_network.train() \n",
    "            #return np.argmax(action_values.cpu().data.numpy())\n",
    "            action_values = action_values.cpu().data.numpy()\n",
    "            #print('NN CT', action_values)\n",
    "            return action_values\n",
    "        else:\n",
    "            # Return a random action for random value > eps\n",
    "            # return random.choice(np.arange(self.action_size))\n",
    "            action_values = np.array([np.random.uniform(0,1),np.random.uniform(-1,1)]).reshape(1,-1)\n",
    "            #print('Rand act', action_values)\n",
    "            return action_values\n",
    "        \n",
    "    def update_model(self):\n",
    "        '''\n",
    "        Update the Q-network based on a batch of experiences from the replay memory.\n",
    "        '''\n",
    "        # Sample a batch of experiences from memory\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        # Get Q-values for the actions that were actually taken\n",
    "        #q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        q_values = self.q_network(states)\n",
    "        \n",
    "        # Get maximum Q-value for the next states from target network\n",
    "        #next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        next_q_values = self.target_network(next_states).detach()\n",
    "        \n",
    "        # Compute the expected Q-values\n",
    "        #print(rewards.repeat(1, 2).reshape(-1, 2).shape, next_q_values.shape, dones.shape)\n",
    "        expected_q_values = rewards.repeat(1, 2).reshape(-1, 2) + self.gamma * next_q_values * (1 - dones.repeat(1, 2).reshape(-1, 2))\n",
    "\n",
    "        # Compute the loss between the current and expected Q values\n",
    "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
    "        \n",
    "        # Zero all gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step the optimizer\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        '''\n",
    "        Update the weights of the target network to match those of the Q-network.\n",
    "        '''\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tScore: -90.00, done: True\n",
      "Episode 1\tAverage Score: -90.00Episode 2\tScore: 351.13, done: True\n",
      "Episode 2\tAverage Score: 130.57Episode 3\tScore: 230.04, done: True\n",
      "Episode 3\tAverage Score: 163.73Episode 4\tScore: 30.50, done: True\n",
      "Episode 4\tAverage Score: 130.42Episode 5\tScore: 250.53, done: True\n",
      "Episode 5\tAverage Score: 154.44Episode 6\tScore: 819.55, done: True\n",
      "Episode 6\tAverage Score: 265.29Episode 7\tScore: 350.64, done: True\n",
      "Episode 7\tAverage Score: 277.49Episode 8\tScore: 347.23, done: True\n",
      "Episode 8\tAverage Score: 286.20Episode 9\tScore: 407.53, done: True\n",
      "Episode 9\tAverage Score: 299.68Episode 10\tScore: 369.30, done: True\n",
      "Episode 10\tAverage Score: 306.65\n",
      "Episode 11\tScore: -4.31, done: True\n",
      "Episode 11\tAverage Score: 278.38Episode 12\tScore: 1193.47, done: True\n",
      "Episode 12\tAverage Score: 354.64Episode 13\tScore: -74.21, done: True\n",
      "Episode 13\tAverage Score: 321.65Episode 14\tScore: -65.57, done: True\n",
      "Episode 14\tAverage Score: 293.99Episode 15\tScore: 1213.83, done: True\n",
      "Episode 15\tAverage Score: 355.31Episode 16\tScore: 47.31, done: True\n",
      "Episode 16\tAverage Score: 336.06Episode 17\tScore: 1234.64, done: True\n",
      "Episode 17\tAverage Score: 388.92Episode 18\tScore: 73.23, done: True\n",
      "Episode 18\tAverage Score: 371.38Episode 19\tScore: 1079.11, done: True\n",
      "Episode 19\tAverage Score: 408.63Episode 20\tScore: 187.15, done: True\n",
      "Episode 20\tAverage Score: 397.56\n",
      "Episode 21\tScore: 156.37, done: True\n",
      "Episode 21\tAverage Score: 386.07Episode 22\tScore: 89.14, done: True\n",
      "Episode 22\tAverage Score: 372.57Episode 23\tScore: 56.11, done: True\n",
      "Episode 23\tAverage Score: 358.81Episode 24\tScore: 25.04, done: True\n",
      "Episode 24\tAverage Score: 344.91Episode 25\tScore: -47.61, done: True\n",
      "Episode 25\tAverage Score: 329.21Episode 26\tScore: 333.56, done: True\n",
      "Episode 26\tAverage Score: 329.37Episode 27\tScore: 47.76, done: True\n",
      "Episode 27\tAverage Score: 318.94Episode 28\tScore: 550.64, done: True\n",
      "Episode 28\tAverage Score: 327.22Episode 29\tScore: -63.41, done: True\n",
      "Episode 29\tAverage Score: 313.75Episode 30\tScore: 363.44, done: True\n",
      "Episode 30\tAverage Score: 315.41\n",
      "Episode 31\tScore: 113.78, done: True\n",
      "Episode 31\tAverage Score: 308.90Episode 32\tScore: 194.46, done: True\n",
      "Episode 32\tAverage Score: 305.32Episode 33\tScore: 210.05, done: True\n",
      "Episode 33\tAverage Score: 302.44Episode 34\tScore: 175.90, done: True\n",
      "Episode 34\tAverage Score: 298.72Episode 35\tScore: 439.37, done: True\n",
      "Episode 35\tAverage Score: 302.73Episode 36\tScore: 269.09, done: True\n",
      "Episode 36\tAverage Score: 301.80Episode 37\tScore: 78.66, done: True\n",
      "Episode 37\tAverage Score: 295.77Episode 38\tScore: 8.56, done: True\n",
      "Episode 38\tAverage Score: 288.21Episode 39\tScore: 363.86, done: True\n",
      "Episode 39\tAverage Score: 290.15Episode 40\tScore: -86.18, done: True\n",
      "Episode 40\tAverage Score: 280.74\n",
      "Episode 41\tScore: 47.86, done: True\n",
      "Episode 41\tAverage Score: 275.06Episode 42\tScore: 115.78, done: True\n",
      "Episode 42\tAverage Score: 271.27Episode 43\tScore: 166.58, done: True\n",
      "Episode 43\tAverage Score: 268.84Episode 44\tScore: -88.12, done: True\n",
      "Episode 44\tAverage Score: 260.72Episode 45\tScore: 217.34, done: True\n",
      "Episode 45\tAverage Score: 259.76Episode 46\tScore: 707.11, done: True\n",
      "Episode 46\tAverage Score: 269.48Episode 47\tScore: 7.39, done: True\n",
      "Episode 47\tAverage Score: 263.91Episode 48\tScore: 230.74, done: True\n",
      "Episode 48\tAverage Score: 263.22Episode 49\tScore: 58.52, done: True\n",
      "Episode 49\tAverage Score: 259.04Episode 50\tScore: 209.92, done: True\n",
      "Episode 50\tAverage Score: 258.06\n",
      "Episode 51\tScore: 1.72, done: True\n",
      "Episode 51\tAverage Score: 253.03Episode 52\tScore: 466.21, done: True\n",
      "Episode 52\tAverage Score: 257.13Episode 53\tScore: 248.78, done: True\n",
      "Episode 53\tAverage Score: 256.97Episode 54\tScore: -37.80, done: True\n",
      "Episode 54\tAverage Score: 251.51Episode 55\tScore: 249.44, done: True\n",
      "Episode 55\tAverage Score: 251.48Episode 56\tScore: 620.99, done: True\n",
      "Episode 56\tAverage Score: 258.07Episode 57\tScore: 229.82, done: True\n",
      "Episode 57\tAverage Score: 257.58Episode 58\tScore: 293.98, done: True\n",
      "Episode 58\tAverage Score: 258.21Episode 59\tScore: 87.46, done: True\n",
      "Episode 59\tAverage Score: 255.31Episode 60\tScore: 942.22, done: True\n",
      "Episode 60\tAverage Score: 266.76\n",
      "Episode 61\tScore: 514.03, done: True\n",
      "Episode 61\tAverage Score: 270.81Episode 62\tScore: -49.19, done: True\n",
      "Episode 62\tAverage Score: 265.65Episode 63\tScore: 316.31, done: True\n",
      "Episode 63\tAverage Score: 266.46Episode 64\tScore: 299.04, done: True\n",
      "Episode 64\tAverage Score: 266.97Episode 65\tScore: 45.09, done: True\n",
      "Episode 65\tAverage Score: 263.55Episode 66\tScore: 456.54, done: True\n",
      "Episode 66\tAverage Score: 266.48Episode 67\tScore: 1241.01, done: True\n",
      "Episode 67\tAverage Score: 281.02Episode 68\tScore: -19.93, done: True\n",
      "Episode 68\tAverage Score: 276.60Episode 69\tScore: -69.42, done: True\n",
      "Episode 69\tAverage Score: 271.58Episode 70\tScore: 94.08, done: True\n",
      "Episode 70\tAverage Score: 269.05\n",
      "Episode 71\tScore: 22.39, done: True\n",
      "Episode 71\tAverage Score: 265.57Episode 72\tScore: 86.59, done: True\n",
      "Episode 72\tAverage Score: 263.09Episode 73\tScore: -83.98, done: True\n",
      "Episode 73\tAverage Score: 258.33Episode 74\tScore: 687.68, done: True\n",
      "Episode 74\tAverage Score: 264.13Episode 75\tScore: 592.25, done: True\n",
      "Episode 75\tAverage Score: 268.51Episode 76\tScore: -28.96, done: True\n",
      "Episode 76\tAverage Score: 264.59Episode 77\tScore: 775.91, done: True\n",
      "Episode 77\tAverage Score: 271.24Episode 78\tScore: 452.25, done: True\n",
      "Episode 78\tAverage Score: 273.56Episode 79\tScore: 316.93, done: True\n",
      "Episode 79\tAverage Score: 274.10Episode 80\tScore: 818.47, done: True\n",
      "Episode 80\tAverage Score: 280.91\n",
      "Episode 81\tScore: 663.67, done: True\n",
      "Episode 81\tAverage Score: 285.63Episode 82\tScore: 6.51, done: True\n",
      "Episode 82\tAverage Score: 282.23Episode 83\tScore: 643.41, done: True\n",
      "Episode 83\tAverage Score: 286.58Episode 84\tScore: 319.69, done: True\n",
      "Episode 84\tAverage Score: 286.98Episode 85\tScore: 247.75, done: True\n",
      "Episode 85\tAverage Score: 286.51Episode 86\tScore: 397.78, done: True\n",
      "Episode 86\tAverage Score: 287.81Episode 87\tScore: -24.10, done: True\n",
      "Episode 87\tAverage Score: 284.22Episode 88\tScore: -75.70, done: True\n",
      "Episode 88\tAverage Score: 280.13Episode 89\tScore: -62.95, done: True\n",
      "Episode 89\tAverage Score: 276.28Episode 90\tScore: -58.35, done: True\n",
      "Episode 90\tAverage Score: 272.56\n",
      "Episode 91\tScore: 175.35, done: True\n",
      "Episode 91\tAverage Score: 271.49Episode 92\tScore: 433.24, done: True\n",
      "Episode 92\tAverage Score: 273.25Episode 93\tScore: 290.56, done: True\n",
      "Episode 93\tAverage Score: 273.44Episode 94\tScore: 643.65, done: True\n",
      "Episode 94\tAverage Score: 277.38Episode 95\tScore: 588.44, done: True\n",
      "Episode 95\tAverage Score: 280.65Episode 96\tScore: 912.48, done: True\n",
      "Episode 96\tAverage Score: 287.23Episode 97\tScore: 1395.08, done: True\n",
      "Episode 97\tAverage Score: 298.65Episode 98\tScore: -34.70, done: True\n",
      "Episode 98\tAverage Score: 295.25Episode 99\tScore: 95.60, done: True\n",
      "Episode 99\tAverage Score: 293.23Episode 100\tScore: 93.61, done: True\n",
      "Episode 100\tAverage Score: 291.24\n"
     ]
    }
   ],
   "source": [
    "def train(agent, env, n_episodes=2000, n_max_step=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, target_update=10):\n",
    "    '''\n",
    "    Train a DQN agent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    agent: DQNAgent\n",
    "        The agent to be trained.\n",
    "    env: gym.Env\n",
    "        The environment in which the agent is trained.\n",
    "    n_episodes: int, default=2000\n",
    "        The number of episodes for which to train the agent.\n",
    "    eps_start: float, default=1.0\n",
    "        The starting epsilon for epsilon-greedy action selection.\n",
    "    eps_end: float, default=0.01\n",
    "        The minimum value that epsilon can reach.\n",
    "    eps_decay: float, default=0.995\n",
    "        The decay rate for epsilon after each episode.\n",
    "    target_update: int, default=10\n",
    "        The frequency (number of episodes) with which the target network should be updated.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of float\n",
    "        The total reward obtained in each episode.\n",
    "    '''\n",
    "\n",
    "    # Initialize the scores list and scores window\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "\n",
    "    # Loop over episodes\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        # Reset environment and score at the start of each episode\n",
    "        state = env.reset()\n",
    "        score = 0 \n",
    "\n",
    "        # Loop over steps\n",
    "        for n_step in range(n_max_step):\n",
    "            \n",
    "            # Select an action using current agent policy then apply in environment\n",
    "            action = agent.act(state, eps)\n",
    "            #print(action)\n",
    "            next_state, reward, terminated, raw_reward = env.step(action) \n",
    "            done = terminated\n",
    "            \n",
    "            # Update the agent, state and score\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state \n",
    "            score += reward\n",
    "\n",
    "            # End the episode if done\n",
    "            if done:\n",
    "                break \n",
    "        print(f\"Episode {i_episode}\\tScore: {score:.2f}, done: {done}\") \n",
    "        # At the end of episode append and save scores\n",
    "        scores_window.append(score)\n",
    "        scores.append(score) \n",
    "\n",
    "        # Decrease epsilon\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "\n",
    "        # Print some info\n",
    "        print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\", end=\"\")\n",
    "\n",
    "        # Update target network every target_update episodes\n",
    "        if i_episode % target_update == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "        # Print average score every 100 episodes\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # This environment is considered to be solved for a mean score of 200 or greater, so stop training.\n",
    "        if i_episode % 100 == 0 and np.mean(scores_window) >= 200:\n",
    "            break\n",
    "            \n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Make an environment\n",
    "env = SimpleUAVEnv()\n",
    "state_size = 18\n",
    "action_size = 2\n",
    "\n",
    "# Initilize a DQN agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Train it\n",
    "scores = train(agent, env, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_network(torch.tensor(state, dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0616, -0.2403], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.target_network(torch.tensor(state, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NN in Jax/Flax**\n",
    "pip install flax (CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef train(model_state, episodes=MAX_EPISODES, steps=1000):\\n    memory = deque(maxlen=2000)\\n    last_100_scores = deque(maxlen=100)\\n    epsilon = 1.0\\n    results = []\\n    acceptable_avg_score_counter = 0\\n\\n    def record_episode(episode_id, step_id, score, results):\\n        nonlocal last_100_scores, acceptable_avg_score_counter\\n\\n        last_100_scores.append(score)\\n        mean_last_100_scores = np.mean(last_100_scores)\\n\\n        if mean_last_100_scores > ACCEPTABLE_AVERAGE_SCORE_THRESHOLD:\\n            acceptable_avg_score_counter += 1\\n        else:\\n            acceptable_avg_score_counter = 0\\n\\n        print(\\n            'Episode:', episode_id,\\n            'Steps:', step_id,\\n            'Score:', score,\\n            '100 Rolling Average:', mean_last_100_scores,\\n            'Acceptable Avg Score Count:', acceptable_avg_score_counter,\\n        )\\n\\n        results.append({\\n            'episode': episode_id,\\n            'steps': step_id,\\n            'score': score,\\n            'rolling_avg_score': mean_last_100_scores\\n        })\\n\\n\\n    for episode_id in range(episodes):\\n        state = env.reset()\\n        score = 0\\n\\n        for step_id in range(steps):\\n            action = choose_action(state, model_state.params, epsilon)\\n            next_state, reward, done, _ = env.step(action)\\n            if done:\\n                next_state = np.zeros(STATE_SIZE)\\n            memory.append((state, action, reward, next_state))\\n            state = next_state if next_state is not None else np.zeros(STATE_SIZE)\\n            score += reward\\n\\n            if len(memory) >= BATCH_SIZE:\\n                batch = random.sample(memory, BATCH_SIZE)\\n                states, actions, rewards, next_states = map(np.array, zip(*batch))\\n                #print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\\n                model_state, _ = train_step(model_state, (states, actions, rewards, next_states), GAMMA)\\n\\n            if done:\\n                record_episode(episode_id, step_id, score, results)\\n                break\\n\\n            epsilon *= DECAY_FACTOR\\n\\n        if acceptable_avg_score_counter >= MAX_ACCEPTABLE_AVG_SCORE_COUNTER:\\n            break\\n        #print('Epsilon:', epsilon)\\n\\n    return model_state, results\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBS_ARR_START_IDX = 8 # The first 8 elements are the UAV's position, velocity, angle, angular velocity, dist2goal, and angle2goal\n",
    "STATE_SIZE = OBS_ARR_START_IDX + UAV_NUM_RAYS\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    input_dim: int\n",
    "    latent_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.input_dim)(x)  \n",
    "        x = nn.silu(x)        \n",
    "        x = nn.Dense(self.latent_dim)(x)   \n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dense(self.latent_dim)(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "#model = ANN()\n",
    "#params =model.init(key, jnp.ones((1, STATE_SIZE)), STATE_SIZE, 64, 2)\n",
    "model = ANN(input_dim=STATE_SIZE, latent_dim=64, output_dim=2)\n",
    "params =model.init(key, jnp.ones((1, STATE_SIZE)))\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Create the training state\n",
    "train_state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "\n",
    "GAMMA = .99\n",
    "DECAY_FACTOR = .99995\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPISODES = 100\n",
    "#MAX_EPISODES = 2\n",
    "ACCEPTABLE_AVERAGE_SCORE_THRESHOLD = 190\n",
    "MAX_ACCEPTABLE_AVG_SCORE_COUNTER = 100\n",
    "\n",
    "# Function to select an action\n",
    "def choose_action(state, params, epsilon):\n",
    "    def random_action():\n",
    "        return np.array([np.random.uniform(0, 1), np.random.uniform(-1, 1)])\n",
    "\n",
    "    def predicted_action():\n",
    "        q_values = model.apply(params, jnp.expand_dims(jnp.array(state), 0))\n",
    "        action = np.argmax(q_values, axis=-1)\n",
    "        return action\n",
    "\n",
    "    return random_action() if np.random.random() <= epsilon else predicted_action()\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, gamma):\n",
    "    def loss_fn(params):\n",
    "        # Extracting experiences from the batch\n",
    "        states, actions, rewards, next_states = batch\n",
    "        q_values = model.apply(params, states)\n",
    "        next_q_values = model.apply(params, next_states)\n",
    "        max_next_q_values = jnp.max(next_q_values, axis=1)\n",
    "        target_q_values = rewards + gamma * max_next_q_values\n",
    "\n",
    "        # Q-value for the action that was taken\n",
    "        actions_one_hot = jax.nn.one_hot(actions, 2)  # Assuming 2 actions\n",
    "        q_action = jnp.sum(q_values * actions_one_hot, axis=1)\n",
    "\n",
    "        # Loss is MSE between q_action and target_q_values\n",
    "        return jnp.mean((target_q_values - q_action) ** 2)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "'''\n",
    "def train(model_state, episodes=MAX_EPISODES, steps=1000):\n",
    "    memory = deque(maxlen=2000)\n",
    "    last_100_scores = deque(maxlen=100)\n",
    "    epsilon = 1.0\n",
    "    results = []\n",
    "    acceptable_avg_score_counter = 0\n",
    "\n",
    "    def record_episode(episode_id, step_id, score, results):\n",
    "        nonlocal last_100_scores, acceptable_avg_score_counter\n",
    "\n",
    "        last_100_scores.append(score)\n",
    "        mean_last_100_scores = np.mean(last_100_scores)\n",
    "\n",
    "        if mean_last_100_scores > ACCEPTABLE_AVERAGE_SCORE_THRESHOLD:\n",
    "            acceptable_avg_score_counter += 1\n",
    "        else:\n",
    "            acceptable_avg_score_counter = 0\n",
    "\n",
    "        print(\n",
    "            'Episode:', episode_id,\n",
    "            'Steps:', step_id,\n",
    "            'Score:', score,\n",
    "            '100 Rolling Average:', mean_last_100_scores,\n",
    "            'Acceptable Avg Score Count:', acceptable_avg_score_counter,\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'episode': episode_id,\n",
    "            'steps': step_id,\n",
    "            'score': score,\n",
    "            'rolling_avg_score': mean_last_100_scores\n",
    "        })\n",
    "\n",
    "\n",
    "    for episode_id in range(episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "\n",
    "        for step_id in range(steps):\n",
    "            action = choose_action(state, model_state.params, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                next_state = np.zeros(STATE_SIZE)\n",
    "            memory.append((state, action, reward, next_state))\n",
    "            state = next_state if next_state is not None else np.zeros(STATE_SIZE)\n",
    "            score += reward\n",
    "\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                batch = random.sample(memory, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states = map(np.array, zip(*batch))\n",
    "                #print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\n",
    "                model_state, _ = train_step(model_state, (states, actions, rewards, next_states), GAMMA)\n",
    "\n",
    "            if done:\n",
    "                record_episode(episode_id, step_id, score, results)\n",
    "                break\n",
    "\n",
    "            epsilon *= DECAY_FACTOR\n",
    "\n",
    "        if acceptable_avg_score_counter >= MAX_ACCEPTABLE_AVG_SCORE_COUNTER:\n",
    "            break\n",
    "        #print('Epsilon:', epsilon)\n",
    "\n",
    "    return model_state, results\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Steps: 0 Score: -99.78322177659737 100 Rolling Average: -99.78322177659737 Acceptable Avg Score Count: 0\n",
      "Episode: 1 Steps: 0 Score: -99.58036442427931 100 Rolling Average: -99.68179310043834 Acceptable Avg Score Count: 0\n",
      "Episode: 2 Steps: 0 Score: -99.2921602751232 100 Rolling Average: -99.55191549199996 Acceptable Avg Score Count: 0\n",
      "Episode: 3 Steps: 0 Score: -99.41187735174036 100 Rolling Average: -99.51690595693506 Acceptable Avg Score Count: 0\n",
      "Episode: 4 Steps: 0 Score: -99.58869705122409 100 Rolling Average: -99.53126417579287 Acceptable Avg Score Count: 0\n",
      "Episode: 5 Steps: 0 Score: -99.43245130109646 100 Rolling Average: -99.51479536334347 Acceptable Avg Score Count: 0\n",
      "Episode: 6 Steps: 0 Score: -98.30110407314451 100 Rolling Average: -99.34141089331504 Acceptable Avg Score Count: 0\n",
      "Episode: 7 Steps: 0 Score: -99.90439416594263 100 Rolling Average: -99.4117838023935 Acceptable Avg Score Count: 0\n",
      "Episode: 8 Steps: 0 Score: -99.6392169815797 100 Rolling Average: -99.43705415563642 Acceptable Avg Score Count: 0\n",
      "Episode: 9 Steps: 0 Score: -99.17083697543869 100 Rolling Average: -99.41043243761663 Acceptable Avg Score Count: 0\n",
      "Episode: 10 Steps: 0 Score: -99.41159526564886 100 Rolling Average: -99.41053814925591 Acceptable Avg Score Count: 0\n",
      "Episode: 11 Steps: 0 Score: -99.32970844887596 100 Rolling Average: -99.40380234089092 Acceptable Avg Score Count: 0\n",
      "Episode: 12 Steps: 0 Score: -99.49920540869093 100 Rolling Average: -99.41114103841399 Acceptable Avg Score Count: 0\n",
      "Episode: 13 Steps: 0 Score: -98.40365015351244 100 Rolling Average: -99.33917740377817 Acceptable Avg Score Count: 0\n",
      "Episode: 14 Steps: 0 Score: -99.40717684162676 100 Rolling Average: -99.34371069963474 Acceptable Avg Score Count: 0\n",
      "Episode: 15 Steps: 0 Score: -100.175487372015 100 Rolling Average: -99.39569674165853 Acceptable Avg Score Count: 0\n",
      "Episode: 16 Steps: 0 Score: -99.8963614640937 100 Rolling Average: -99.42514760768412 Acceptable Avg Score Count: 0\n",
      "Episode: 17 Steps: 0 Score: -99.79773306759914 100 Rolling Average: -99.44584679990163 Acceptable Avg Score Count: 0\n",
      "Episode: 18 Steps: 0 Score: -99.9271025262487 100 Rolling Average: -99.47117604865673 Acceptable Avg Score Count: 0\n",
      "Episode: 19 Steps: 0 Score: -99.31562886243357 100 Rolling Average: -99.46339868934558 Acceptable Avg Score Count: 0\n",
      "Episode: 20 Steps: 0 Score: -99.6015312851131 100 Rolling Average: -99.46997643200118 Acceptable Avg Score Count: 0\n",
      "Episode: 21 Steps: 0 Score: -97.59100161792371 100 Rolling Average: -99.38456848590675 Acceptable Avg Score Count: 0\n",
      "Episode: 22 Steps: 0 Score: -99.54373683221941 100 Rolling Average: -99.39148884878992 Acceptable Avg Score Count: 0\n",
      "Episode: 23 Steps: 0 Score: -97.27178608282671 100 Rolling Average: -99.30316790020811 Acceptable Avg Score Count: 0\n",
      "Episode: 24 Steps: 0 Score: -99.60765888645726 100 Rolling Average: -99.31534753965806 Acceptable Avg Score Count: 0\n",
      "Episode: 25 Steps: 0 Score: -98.82718670232987 100 Rolling Average: -99.29657212283776 Acceptable Avg Score Count: 0\n",
      "Episode: 26 Steps: 0 Score: -99.7234812522983 100 Rolling Average: -99.31238357207704 Acceptable Avg Score Count: 0\n",
      "Episode: 27 Steps: 0 Score: -99.46661431499044 100 Rolling Average: -99.31789181289537 Acceptable Avg Score Count: 0\n",
      "Episode: 28 Steps: 0 Score: -99.63022661360037 100 Rolling Average: -99.32866197843693 Acceptable Avg Score Count: 0\n",
      "Episode: 29 Steps: 0 Score: -98.93025166667978 100 Rolling Average: -99.31538163471168 Acceptable Avg Score Count: 0\n",
      "Episode: 30 Steps: 0 Score: -98.94546730013698 100 Rolling Average: -99.30344891424153 Acceptable Avg Score Count: 0\n",
      "Episode: 31 Steps: 0 Score: -99.51968477921558 100 Rolling Average: -99.31020628502196 Acceptable Avg Score Count: 0\n",
      "Episode: 32 Steps: 0 Score: -99.69050737363129 100 Rolling Average: -99.32173056043436 Acceptable Avg Score Count: 0\n",
      "Episode: 33 Steps: 0 Score: -99.44828452249209 100 Rolling Average: -99.325452735789 Acceptable Avg Score Count: 0\n",
      "Episode: 34 Steps: 0 Score: -99.87223012055009 100 Rolling Average: -99.34107494678217 Acceptable Avg Score Count: 0\n",
      "Episode: 35 Steps: 0 Score: -99.45373574921658 100 Rolling Average: -99.34420441351648 Acceptable Avg Score Count: 0\n",
      "Episode: 36 Steps: 0 Score: -99.68120191437406 100 Rolling Average: -99.3533124540802 Acceptable Avg Score Count: 0\n",
      "Episode: 37 Steps: 0 Score: -99.3062268281085 100 Rolling Average: -99.3520733586599 Acceptable Avg Score Count: 0\n",
      "Episode: 38 Steps: 0 Score: -99.43933491499918 100 Rolling Average: -99.35431083446346 Acceptable Avg Score Count: 0\n",
      "Episode: 39 Steps: 0 Score: -99.64121919001455 100 Rolling Average: -99.36148354335224 Acceptable Avg Score Count: 0\n",
      "Episode: 40 Steps: 0 Score: -99.444610420668 100 Rolling Average: -99.36351102816482 Acceptable Avg Score Count: 0\n",
      "Episode: 41 Steps: 0 Score: -100.14980847730266 100 Rolling Average: -99.38223239600144 Acceptable Avg Score Count: 0\n",
      "Episode: 42 Steps: 0 Score: -98.68764117591319 100 Rolling Average: -99.36607911181333 Acceptable Avg Score Count: 0\n",
      "Episode: 43 Steps: 0 Score: -99.77407461232582 100 Rolling Average: -99.37535173682498 Acceptable Avg Score Count: 0\n",
      "Episode: 44 Steps: 0 Score: -100.00122669946334 100 Rolling Average: -99.38926006932806 Acceptable Avg Score Count: 0\n",
      "Episode: 45 Steps: 0 Score: -99.84652323819489 100 Rolling Average: -99.39920057299908 Acceptable Avg Score Count: 0\n",
      "Episode: 46 Steps: 0 Score: -99.57223570288379 100 Rolling Average: -99.40288217150726 Acceptable Avg Score Count: 0\n",
      "Episode: 47 Steps: 0 Score: -99.5103169500901 100 Rolling Average: -99.40512039606108 Acceptable Avg Score Count: 0\n",
      "Episode: 48 Steps: 0 Score: -99.76555108382301 100 Rolling Average: -99.41247612438275 Acceptable Avg Score Count: 0\n",
      "Episode: 49 Steps: 0 Score: -99.62696048893457 100 Rolling Average: -99.4167658116738 Acceptable Avg Score Count: 0\n",
      "Episode: 50 Steps: 0 Score: -99.96056534615744 100 Rolling Average: -99.42742854764407 Acceptable Avg Score Count: 0\n",
      "Episode: 51 Steps: 0 Score: -97.20245874585767 100 Rolling Average: -99.38464066684047 Acceptable Avg Score Count: 0\n",
      "Episode: 52 Steps: 0 Score: -99.58671858962472 100 Rolling Average: -99.3884534578364 Acceptable Avg Score Count: 0\n",
      "Episode: 53 Steps: 0 Score: -99.3044833358136 100 Rolling Average: -99.38689845557673 Acceptable Avg Score Count: 0\n",
      "Episode: 54 Steps: 0 Score: -99.82090183195947 100 Rolling Average: -99.3947894260564 Acceptable Avg Score Count: 0\n",
      "Episode: 55 Steps: 0 Score: -99.13974369374442 100 Rolling Average: -99.3902350379794 Acceptable Avg Score Count: 0\n",
      "Episode: 56 Steps: 0 Score: -99.76794038820657 100 Rolling Average: -99.3968614476325 Acceptable Avg Score Count: 0\n",
      "Episode: 57 Steps: 0 Score: -99.42865597174645 100 Rolling Average: -99.39740962908274 Acceptable Avg Score Count: 0\n",
      "Episode: 58 Steps: 0 Score: -99.53004318986835 100 Rolling Average: -99.39965765553674 Acceptable Avg Score Count: 0\n",
      "Episode: 59 Steps: 0 Score: -99.48009584914145 100 Rolling Average: -99.40099829209682 Acceptable Avg Score Count: 0\n",
      "Episode: 60 Steps: 0 Score: -99.41412780890887 100 Rolling Average: -99.40121353007734 Acceptable Avg Score Count: 0\n",
      "Episode: 61 Steps: 0 Score: -99.30930492355989 100 Rolling Average: -99.39973113319803 Acceptable Avg Score Count: 0\n",
      "Episode: 62 Steps: 0 Score: -99.60945725177058 100 Rolling Average: -99.40306011920711 Acceptable Avg Score Count: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(64, 2), (64, 2, 2)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/util.py:263\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/util.py:256\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/lax/lax.py:152\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m@cache\u001b[39m()\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_broadcast_shapes_cached\u001b[39m(\u001b[39m*\u001b[39mshapes: \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m--> 152\u001b[0m   \u001b[39mreturn\u001b[39;00m _broadcast_shapes_uncached(\u001b[39m*\u001b[39;49mshapes)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64, 2), (64, 2, 2)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tonyz/myProj/AiProj/uav.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     states, actions, rewards, next_states \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(np\u001b[39m.\u001b[39marray, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m#print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     model_state, _ \u001b[39m=\u001b[39m train_step(model_state, (states, actions, rewards, next_states), GAMMA)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     record_episode(episode_id, step_id, score, results)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "\u001b[1;32m/home/tonyz/myProj/AiProj/uav.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean((target_q_values \u001b[39m-\u001b[39m q_action) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m grad_fn \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvalue_and_grad(loss_fn)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m loss, grads \u001b[39m=\u001b[39m grad_fn(state\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mreturn\u001b[39;00m state\u001b[39m.\u001b[39mapply_gradients(grads\u001b[39m=\u001b[39mgrads), loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "\u001b[1;32m/home/tonyz/myProj/AiProj/uav.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Q-value for the action that was taken\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m actions_one_hot \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mone_hot(actions, \u001b[39m2\u001b[39m)  \u001b[39m# Assuming 2 actions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m q_action \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msum(q_values \u001b[39m*\u001b[39;49m actions_one_hot, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Loss is MSE between q_action and target_q_values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/tonyz/myProj/AiProj/uav.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean((target_q_values \u001b[39m-\u001b[39m q_action) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:728\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mop\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 728\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maval, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:256\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    254\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    257\u001b[0m \u001b[39m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(other) \u001b[39min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:96\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(x1, x2, \u001b[39m/\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m   x1, x2 \u001b[39m=\u001b[39m promote_args(numpy_fn\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, x1, x2)\n\u001b[1;32m     97\u001b[0m   \u001b[39mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[39mif\u001b[39;00m x1\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mbool_ \u001b[39melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/util.py:363\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    361\u001b[0m check_arraylike(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[1;32m    362\u001b[0m _check_no_float0s(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m promote_shapes(fun_name, \u001b[39m*\u001b[39;49mpromote_dtypes(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/numpy/util.py:248\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mnumpy_rank_promotion\u001b[39m.\u001b[39mvalue \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 248\u001b[0m result_rank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(lax\u001b[39m.\u001b[39;49mbroadcast_shapes(\u001b[39m*\u001b[39;49mshapes))\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (result_rank \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(shp)) \u001b[39m+\u001b[39m shp)\n\u001b[1;32m    250\u001b[0m         \u001b[39mfor\u001b[39;00m arg, shp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gym/lib/python3.11/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    166\u001b[0m result_shape \u001b[39m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(64, 2), (64, 2, 2)]"
     ]
    }
   ],
   "source": [
    "episodes =100\n",
    "steps = 1000\n",
    "memory = deque(maxlen=2000)\n",
    "last_100_scores = deque(maxlen=100)\n",
    "epsilon = 1.0\n",
    "results = []\n",
    "acceptable_avg_score_counter = 0\n",
    "model_state = train_state\n",
    "\n",
    "def record_episode(episode_id, step_id, score, results):\n",
    "    last_100_scores.append(score)\n",
    "    mean_last_100_scores = np.mean(last_100_scores)\n",
    "\n",
    "    if mean_last_100_scores > ACCEPTABLE_AVERAGE_SCORE_THRESHOLD:\n",
    "        acceptable_avg_score_counter += 1\n",
    "    else:\n",
    "        acceptable_avg_score_counter = 0\n",
    "\n",
    "    print(\n",
    "        'Episode:', episode_id,\n",
    "        'Steps:', step_id,\n",
    "        'Score:', score,\n",
    "        '100 Rolling Average:', mean_last_100_scores,\n",
    "        'Acceptable Avg Score Count:', acceptable_avg_score_counter,\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        'episode': episode_id,\n",
    "        'steps': step_id,\n",
    "        'score': score,\n",
    "        'rolling_avg_score': mean_last_100_scores\n",
    "    })\n",
    "\n",
    "\n",
    "for episode_id in range(episodes):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "\n",
    "    for step_id in range(steps):\n",
    "        action = choose_action(state, model_state.params, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            next_state = np.zeros(STATE_SIZE)\n",
    "        memory.append((state, action, reward, next_state))\n",
    "        state = next_state if next_state is not None else np.zeros(STATE_SIZE)\n",
    "        score += reward\n",
    "\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            batch = random.sample(memory, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states = map(np.array, zip(*batch))\n",
    "            #print(states.dtype, actions.dtype, rewards.dtype, next_states.dtype)\n",
    "            model_state, _ = train_step(model_state, (states, actions, rewards, next_states), GAMMA)\n",
    "\n",
    "        if done:\n",
    "            record_episode(episode_id, step_id, score, results)\n",
    "            break\n",
    "\n",
    "        epsilon *= DECAY_FACTOR\n",
    "\n",
    "    if acceptable_avg_score_counter >= MAX_ACCEPTABLE_AVG_SCORE_COUNTER:\n",
    "        break\n",
    "    #print('Epsilon:', epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 18) (64, 2) (64,) (64, 18)\n"
     ]
    }
   ],
   "source": [
    "print(states.shape, actions.shape, rewards.shape, next_states.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
